# SysArmor Vector - MVP 简化版本
[api]
enabled = true
address = "0.0.0.0:${VECTOR_API_PORT}"

# 接收来自 rsyslog 的 auditd 数据
[sources.tcp_receiver]
type = "socket"
address = "0.0.0.0:${VECTOR_TCP_PORT}"
mode = "tcp"
max_length = 102400
decoding.codec = "json"

# 简化的数据处理 - 基于 event_type 做路由
[transforms.process_and_route]
type = "remap"
inputs = ["tcp_receiver"]
drop_on_error = true
source = '''
# 添加处理时间戳
.processed_at = now()

# 验证必需字段
if !exists(.collector_id) {
    abort "No collector_id found, dropping message"
}

if !exists(.message) {
    abort "No message content found, dropping message"
}

# 使用 collector_id 作为分区键，确保同一 collector 的数据有序
.partition_key = string!(.collector_id)

# 根据 event_type 和 data_source 进行路由
event_type = .event_type || "unknown"
data_source = .data_source || "unknown"

if event_type == "syslog" {
    # syslog 类型数据（主要是 auditd）转发到 raw audit topic
    .target_topic = "sysarmor.raw.audit"
    .data_source = "syslog"
    .event_category = "raw_audit"
} else if data_source == "sysdig" || event_type == "sysdig" {
    # Sysdig 数据直接转发到 events.sysdig topic
    .target_topic = "sysarmor.events.sysdig"
    .data_source = "sysdig"
    .event_category = "events_sysdig"
} else if data_source == "converted_audit" || event_type == "converted_audit" {
    # 转换后的 auditd 数据转发到 events.audit topic
    .target_topic = "sysarmor.events.audit"
    .data_source = "converted_audit"
    .event_category = "events_audit"
} else {
    # 其他未知类型数据转发到 other topic
    .target_topic = "sysarmor.raw.other"
    .data_source = event_type
    .event_category = "raw_other"
}

# 确保 timestamp 存在
if !exists(.timestamp) {
    .timestamp = now()
}
'''

# 条件路由到 raw audit topic
[transforms.route_to_raw_audit]
type = "filter"
inputs = ["process_and_route"]
condition = '.target_topic == "sysarmor.raw.audit"'

# 条件路由到 events sysdig topic
[transforms.route_to_events_sysdig]
type = "filter"
inputs = ["process_and_route"]
condition = '.target_topic == "sysarmor.events.sysdig"'

# 条件路由到 events audit topic
[transforms.route_to_events_audit]
type = "filter"
inputs = ["process_and_route"]
condition = '.target_topic == "sysarmor.events.audit"'

# 条件路由到 other topic
[transforms.route_to_other]
type = "filter"
inputs = ["process_and_route"]
condition = '.target_topic == "sysarmor.raw.other"'

# 输出到 raw audit topic
[sinks.kafka_raw_audit]
type = "kafka"
inputs = ["route_to_raw_audit"]
bootstrap_servers = "${KAFKA_BOOTSTRAP_SERVERS}"
topic = "sysarmor.raw.audit"
key_field = "partition_key"
encoding.codec = "json"
compression = "snappy"

# 输出到 events sysdig topic
[sinks.kafka_events_sysdig]
type = "kafka"
inputs = ["route_to_events_sysdig"]
bootstrap_servers = "${KAFKA_BOOTSTRAP_SERVERS}"
topic = "sysarmor.events.sysdig"
key_field = "partition_key"
encoding.codec = "json"
compression = "snappy"

# 输出到 events audit topic
[sinks.kafka_events_audit]
type = "kafka"
inputs = ["route_to_events_audit"]
bootstrap_servers = "${KAFKA_BOOTSTRAP_SERVERS}"
topic = "sysarmor.events.audit"
key_field = "partition_key"
encoding.codec = "json"
compression = "snappy"

# 输出到 other topic（为未来扩展预留）
[sinks.kafka_raw_other]
type = "kafka"
inputs = ["route_to_other"]
bootstrap_servers = "${KAFKA_BOOTSTRAP_SERVERS}"
topic = "sysarmor.raw.other"
key_field = "partition_key"
encoding.codec = "json"
compression = "snappy"

# Kafka 生产者配置
[sinks.kafka_raw_audit.librdkafka_options]
"message.timeout.ms" = "30000"
"delivery.timeout.ms" = "30000"
"request.timeout.ms" = "30000"
"queue.buffering.max.messages" = "100000"
"queue.buffering.max.kbytes" = "1048576"
"batch.num.messages" = "1000"
"batch.size" = "1000000"
"linger.ms" = "100"
"compression.type" = "snappy"
"acks" = "1"
"retries" = "3"
"retry.backoff.ms" = "100"

# Kafka 生产者配置 - events.sysdig
[sinks.kafka_events_sysdig.librdkafka_options]
"message.timeout.ms" = "30000"
"delivery.timeout.ms" = "30000"
"request.timeout.ms" = "30000"
"queue.buffering.max.messages" = "100000"
"queue.buffering.max.kbytes" = "1048576"
"batch.num.messages" = "1000"
"batch.size" = "1000000"
"linger.ms" = "100"
"compression.type" = "snappy"
"acks" = "1"
"retries" = "3"
"retry.backoff.ms" = "100"

# Kafka 生产者配置 - events.audit
[sinks.kafka_events_audit.librdkafka_options]
"message.timeout.ms" = "30000"
"delivery.timeout.ms" = "30000"
"request.timeout.ms" = "30000"
"queue.buffering.max.messages" = "100000"
"queue.buffering.max.kbytes" = "1048576"
"batch.num.messages" = "1000"
"batch.size" = "1000000"
"linger.ms" = "100"
"compression.type" = "snappy"
"acks" = "1"
"retries" = "3"
"retry.backoff.ms" = "100"

# Kafka 生产者配置 - raw.other
[sinks.kafka_raw_other.librdkafka_options]
"message.timeout.ms" = "30000"
"delivery.timeout.ms" = "30000"
"request.timeout.ms" = "30000"
"queue.buffering.max.messages" = "100000"
"queue.buffering.max.kbytes" = "1048576"
"batch.num.messages" = "1000"
"batch.size" = "1000000"
"linger.ms" = "100"
"compression.type" = "snappy"
"acks" = "1"
"retries" = "3"
"retry.backoff.ms" = "100"

# 内部指标收集
[sources.internal_metrics]
type = "internal_metrics"
scrape_interval_secs = 30

# Prometheus指标导出
[sinks.prometheus_metrics]
type = "prometheus_exporter"
inputs = ["internal_metrics"]
address = "0.0.0.0:${VECTOR_METRICS_PORT}"
default_namespace = "vector"

# 内部日志收集（用于调试）
[sources.internal_logs]
type = "internal_logs"
