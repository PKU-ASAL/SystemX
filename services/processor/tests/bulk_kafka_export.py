#!/usr/bin/env python3
"""
æ‰¹é‡ Kafka å¯¼å‡ºå·¥å…· V2
ä½¿ç”¨ consumer group å’Œ offset ç®¡ç†è¿›è¡Œé«˜æ•ˆæ‰¹é‡å¯¼å‡º
æ¯ä¸ª batch ä½¿ç”¨ç¬¬ä¸€æ¡æ¶ˆæ¯çš„æ—¶é—´æˆ³å‘½åæ–‡ä»¶
"""

import subprocess
import json
import os
import argparse
from datetime import datetime
import time
import tempfile
from dateutil import parser as date_parser

def get_container_name():
    """è·å–æ­£ç¡®çš„ Kafka å®¹å™¨å"""
    try:
        result = subprocess.run(
            ["docker", "ps", "--format", "{{.Names}}"],
            capture_output=True,
            text=True,
            timeout=10
        )
        
        if result.returncode == 0:
            containers = result.stdout.strip().split('\n')
            for container in containers:
                if 'kafka' in container.lower() and 'ui' not in container.lower():
                    return container
        return None
    except Exception:
        return None

def extract_timestamp_from_message(message_json):
    """ä»æ¶ˆæ¯ä¸­æå–æ—¶é—´æˆ³å¹¶æ ¼å¼åŒ–ä¸ºæ–‡ä»¶å"""
    try:
        data = json.loads(message_json)
        timestamp_fields = ['timestamp', 'processed_at', '@timestamp', 'time']
        
        for field in timestamp_fields:
            if field in data:
                timestamp_str = data[field]
                try:
                    if isinstance(timestamp_str, (int, float)):
                        dt = datetime.fromtimestamp(timestamp_str)
                    else:
                        dt = date_parser.parse(timestamp_str)
                    return dt.strftime('%Y%m%d_%H%M%S')
                except:
                    continue
        
        return datetime.now().strftime('%Y%m%d_%H%M%S')
    except:
        return datetime.now().strftime('%Y%m%d_%H%M%S')

def run_kafka_consumer_batch(bootstrap_servers, topic, batch_size, consumer_group, output_dir, batch_num):
    """ä½¿ç”¨ consumer group å¯¼å‡ºä¸€ä¸ªæ‰¹æ¬¡çš„æ•°æ®"""
    
    kafka_container = get_container_name()
    if not kafka_container:
        print("âŒ æœªæ‰¾åˆ° Kafka å®¹å™¨")
        return False, 0, None
    
    print(f"ğŸš€ å¼€å§‹å¯¼å‡ºæ‰¹æ¬¡ {batch_num}...")
    print(f"ğŸ“¡ æœåŠ¡å™¨: {bootstrap_servers}")
    print(f"ğŸ“‹ Topic: {topic}")
    print(f"ğŸ“Š æ‰¹æ¬¡å¤§å°: {batch_size:,}")
    print(f"ğŸ‘¥ Consumer Group: {consumer_group}")
    print(f"ğŸ³ Kafka å®¹å™¨: {kafka_container}")
    
    # æ„å»º Docker å‘½ä»¤ - ä½¿ç”¨ consumer group è‡ªåŠ¨ç®¡ç† offset
    docker_cmd = [
        "docker", "exec", kafka_container,
        "kafka-console-consumer",
        "--bootstrap-server", bootstrap_servers,
        "--topic", topic,
        "--group", consumer_group,  # ä½¿ç”¨ consumer group
        "--max-messages", str(batch_size),
        "--timeout-ms", "60000",  # 1åˆ†é’Ÿè¶…æ—¶
        "--property", "print.offset=false",
        "--property", "print.partition=false",
        "--property", "print.timestamp=false"
    ]
    
    print(f"ğŸ” æ‰§è¡Œå‘½ä»¤: {' '.join(docker_cmd)}")
    
    start_time = time.time()
    
    try:
        # è¿è¡Œå‘½ä»¤å¹¶æ•è·è¾“å‡º
        result = subprocess.run(
            docker_cmd,
            capture_output=True,
            text=True,
            timeout=180  # 3åˆ†é’Ÿæ€»è¶…æ—¶
        )
        
        elapsed_time = time.time() - start_time
        
        if result.returncode == 0:
            # æˆåŠŸå¯¼å‡º
            lines = result.stdout.strip().split('\n')
            
            # è¿‡æ»¤æ‰éJSONè¡Œ
            json_lines = []
            for line in lines:
                line = line.strip()
                if line and line.startswith('{') and line.endswith('}'):
                    try:
                        json.loads(line)  # éªŒè¯JSON
                        json_lines.append(line)
                    except json.JSONDecodeError:
                        continue
            
            if not json_lines:
                print(f"âš ï¸  æ‰¹æ¬¡ {batch_num} æ²¡æœ‰è·å–åˆ°æœ‰æ•ˆæ•°æ®")
                return False, 0, None
            
            # ä½¿ç”¨ç¬¬ä¸€æ¡æ¶ˆæ¯çš„æ—¶é—´æˆ³å‘½åæ–‡ä»¶
            first_message_timestamp = extract_timestamp_from_message(json_lines[0])
            filename = f"{topic}_batch_{batch_num:03d}_{first_message_timestamp}.jsonl"
            output_file = os.path.join(output_dir, filename)
            
            # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
            os.makedirs(output_dir, exist_ok=True)
            
            # å†™å…¥æ–‡ä»¶
            with open(output_file, 'w', encoding='utf-8') as f:
                for line in json_lines:
                    f.write(line + '\n')
            
            # ç»Ÿè®¡ä¿¡æ¯
            message_count = len(json_lines)
            file_size = os.path.getsize(output_file)
            rate = message_count / elapsed_time if elapsed_time > 0 else 0
            
            print(f"\nâœ… æ‰¹æ¬¡ {batch_num} å¯¼å‡ºæˆåŠŸ!")
            print(f"ğŸ“Š æ¶ˆæ¯æ•°é‡: {message_count:,}")
            print(f"â±ï¸  è€—æ—¶: {elapsed_time:.1f} ç§’")
            print(f"ğŸ“ˆ é€Ÿåº¦: {rate:.1f} msg/s")
            print(f"ğŸ“¦ æ–‡ä»¶å¤§å°: {file_size:,} bytes ({file_size/1024/1024:.1f} MB)")
            print(f"ğŸ“ è¾“å‡ºæ–‡ä»¶: {output_file}")
            
            # æ˜¾ç¤ºç¬¬ä¸€æ¡å’Œæœ€åä¸€æ¡æ¶ˆæ¯çš„æ—¶é—´æˆ³
            try:
                first_data = json.loads(json_lines[0])
                last_data = json.loads(json_lines[-1])
                first_ts = first_data.get('timestamp', 'N/A')
                last_ts = last_data.get('timestamp', 'N/A')
                print(f"ğŸ• æ—¶é—´èŒƒå›´: {first_ts} -> {last_ts}")
            except:
                pass
            
            return True, message_count, output_file
            
        else:
            print(f"âŒ æ‰¹æ¬¡ {batch_num} å¯¼å‡ºå¤±è´¥:")
            print(f"   è¿”å›ç : {result.returncode}")
            if result.stderr:
                print(f"   é”™è¯¯è¾“å‡º: {result.stderr}")
            return False, 0, None
            
    except subprocess.TimeoutExpired:
        print(f"âŒ æ‰¹æ¬¡ {batch_num} å¯¼å‡ºè¶…æ—¶ (3åˆ†é’Ÿ)")
        return False, 0, None
    except Exception as e:
        print(f"âŒ æ‰¹æ¬¡ {batch_num} å¯¼å‡ºå¼‚å¸¸: {e}")
        return False, 0, None

def reset_consumer_group(bootstrap_servers, topic, consumer_group):
    """é‡ç½® consumer group åˆ°æœ€æ—©ä½ç½®"""
    kafka_container = get_container_name()
    if not kafka_container:
        return False
    
    print(f"ğŸ”„ é‡ç½® consumer group '{consumer_group}' åˆ°æœ€æ—©ä½ç½®...")
    
    # é‡ç½® consumer group offset åˆ°æœ€æ—©ä½ç½®
    reset_cmd = [
        "docker", "exec", kafka_container,
        "kafka-consumer-groups",
        "--bootstrap-server", bootstrap_servers,
        "--group", consumer_group,
        "--topic", topic,
        "--reset-offsets",
        "--to-earliest",
        "--execute"
    ]
    
    try:
        result = subprocess.run(reset_cmd, capture_output=True, text=True, timeout=30)
        if result.returncode == 0:
            print(f"âœ… Consumer group é‡ç½®æˆåŠŸ")
            return True
        else:
            print(f"âš ï¸  Consumer group é‡ç½®å¤±è´¥: {result.stderr}")
            return True  # ç»§ç»­æ‰§è¡Œï¼Œå¯èƒ½æ˜¯æ–°çš„ group
    except Exception as e:
        print(f"âš ï¸  Consumer group é‡ç½®å¼‚å¸¸: {e}")
        return True  # ç»§ç»­æ‰§è¡Œ

def export_in_batches(bootstrap_servers, topic, total_messages, batch_size, output_dir):
    """åˆ†æ‰¹å¯¼å‡ºå¤§é‡æ•°æ®"""
    
    print(f"ğŸ”§ æ‰¹é‡å¯¼å‡ºæ¨¡å¼")
    if total_messages is None:
        print(f"ğŸ“Š æ€»æ¶ˆæ¯æ•°: æ‰€æœ‰æ¶ˆæ¯ (æ— é™åˆ¶)")
    else:
        print(f"ğŸ“Š æ€»æ¶ˆæ¯æ•°: {total_messages:,}")
    print(f"ğŸ“¦ æ‰¹æ¬¡å¤§å°: {batch_size:,}")
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
    print("=" * 60)
    
    os.makedirs(output_dir, exist_ok=True)
    
    # ç”Ÿæˆå”¯ä¸€çš„ consumer group åç§°
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    consumer_group = f"bulk-export-{timestamp}"
    
    # é‡ç½® consumer group åˆ°æœ€æ—©ä½ç½®
    reset_consumer_group(bootstrap_servers, topic, consumer_group)
    
    total_exported = 0
    batch_num = 1
    exported_files = []
    consecutive_failures = 0
    
    while True:
        # å¦‚æœè®¾ç½®äº†æ€»æ¶ˆæ¯æ•°é™åˆ¶ï¼Œæ£€æŸ¥æ˜¯å¦å·²è¾¾åˆ°
        if total_messages is not None and total_exported >= total_messages:
            print(f"âœ… å·²è¾¾åˆ°ç›®æ ‡æ¶ˆæ¯æ•°: {total_messages:,}")
            break
            
        # è®¡ç®—å½“å‰æ‰¹æ¬¡å¤§å°
        if total_messages is not None:
            remaining = total_messages - total_exported
            current_batch_size = min(batch_size, remaining)
        else:
            current_batch_size = batch_size
        
        print(f"\nğŸ“¦ æ‰¹æ¬¡ {batch_num} (ç›®æ ‡: {current_batch_size:,} æ¡æ¶ˆæ¯)")
        
        # å¯¼å‡ºå½“å‰æ‰¹æ¬¡
        success, count, output_file = run_kafka_consumer_batch(
            bootstrap_servers, topic, current_batch_size, consumer_group, output_dir, batch_num
        )
        
        if success and count > 0:
            total_exported += count
            exported_files.append(output_file)
            batch_num += 1
            consecutive_failures = 0
            
            if count < current_batch_size:
                print(f"âš ï¸  æ‰¹æ¬¡ {batch_num-1} åªå¯¼å‡ºäº† {count:,} æ¡æ¶ˆæ¯ï¼Œå¯èƒ½å·²åˆ°è¾¾æ•°æ®æœ«å°¾")
                break
        else:
            consecutive_failures += 1
            print(f"âŒ æ‰¹æ¬¡ {batch_num} å¯¼å‡ºå¤±è´¥ (è¿ç»­å¤±è´¥: {consecutive_failures})")
            
            if consecutive_failures >= 3:
                print(f"âŒ è¿ç»­å¤±è´¥ {consecutive_failures} æ¬¡ï¼Œåœæ­¢å¯¼å‡º")
                break
            
            # ç­‰å¾…ä¸€ä¸‹å†é‡è¯•
            print(f"â³ ç­‰å¾… 5 ç§’åé‡è¯•...")
            time.sleep(5)
    
    print(f"\nğŸ‰ æ‰¹é‡å¯¼å‡ºå®Œæˆ!")
    print(f"ğŸ“Š æ€»è®¡å¯¼å‡º: {total_exported:,} æ¡æ¶ˆæ¯")
    print(f"ğŸ“¦ æ‰¹æ¬¡æ•°é‡: {len(exported_files)}")
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
    print(f"ğŸ‘¥ Consumer Group: {consumer_group}")
    
    if exported_files:
        print(f"\nğŸ“‹ å¯¼å‡ºæ–‡ä»¶åˆ—è¡¨:")
        total_size = 0
        for i, file_path in enumerate(exported_files, 1):
            file_size = os.path.getsize(file_path)
            total_size += file_size
            print(f"   {i}. {os.path.basename(file_path)} ({file_size:,} bytes)")
        print(f"ğŸ“¦ æ€»æ–‡ä»¶å¤§å°: {total_size:,} bytes ({total_size/1024/1024:.1f} MB)")

def main():
    parser = argparse.ArgumentParser(description='æ‰¹é‡ Kafka å¯¼å‡ºå·¥å…· V2 (ä½¿ç”¨ Consumer Group)')
    parser.add_argument('--bootstrap-servers', 
                       default='localhost:9092',
                       help='Kafka bootstrap servers (é»˜è®¤: localhost:9092)')
    parser.add_argument('--topic', 
                       default='sysarmor-agentless-558c01dd',
                       help='Topic åç§° (é»˜è®¤: sysarmor-agentless-558c01dd)')
    parser.add_argument('--max-messages', 
                       type=int,
                       default=None,
                       help='æœ€å¤§å¯¼å‡ºæ¶ˆæ¯æ•° (é»˜è®¤: å¯¼å‡ºæ‰€æœ‰æ¶ˆæ¯)')
    parser.add_argument('--batch-size', 
                       type=int,
                       default=1000000,
                       help='æ‰¹æ¬¡å¤§å° (é»˜è®¤: 1,000,000)')
    parser.add_argument('--output-dir', 
                       required=True,
                       help='è¾“å‡ºç›®å½• (å¿…éœ€)')
    
    args = parser.parse_args()
    
    print("ğŸ”§ æ‰¹é‡ Kafka å¯¼å‡ºå·¥å…· V2")
    print("=" * 60)
    print(f"ğŸ“¡ Kafka æœåŠ¡å™¨: {args.bootstrap_servers}")
    print(f"ğŸ“‹ Topic: {args.topic}")
    if args.max_messages is None:
        print(f"ğŸ“Š æœ€å¤§æ¶ˆæ¯æ•°: æ‰€æœ‰æ¶ˆæ¯ (æ— é™åˆ¶)")
    else:
        print(f"ğŸ“Š æœ€å¤§æ¶ˆæ¯æ•°: {args.max_messages:,}")
    print(f"ğŸ“¦ æ‰¹æ¬¡å¤§å°: {args.batch_size:,}")
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {args.output_dir}")
    print("=" * 60)
    
    # æ£€æŸ¥ Docker å’Œ Kafka å®¹å™¨
    kafka_container = get_container_name()
    if not kafka_container:
        print("âŒ æœªæ‰¾åˆ°è¿è¡Œä¸­çš„ Kafka å®¹å™¨")
        print("ğŸ’¡ è¯·ç¡®ä¿ Kafka æœåŠ¡æ­£åœ¨è¿è¡Œ")
        return 1
    
    print(f"âœ… æ‰¾åˆ° Kafka å®¹å™¨: {kafka_container}")
    
    # æ‰¹é‡å¯¼å‡º
    export_in_batches(
        args.bootstrap_servers, args.topic, 
        args.max_messages, args.batch_size, args.output_dir
    )
    
    return 0

if __name__ == '__main__':
    exit(main())
