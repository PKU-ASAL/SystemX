#!/usr/bin/env python3
"""
SysArmor Multi-Topic Process Tree Builder for Flink
æ”¯æŒåŒæ—¶å¤„ç†å¤šä¸ª topic (sysarmor-agentless-*) çš„è¿›ç¨‹æ ‘é‡å»º
æ¯ä¸ª topic çš„äº‹ä»¶ç‹¬ç«‹å¤„ç†ï¼Œè¾“å‡ºåˆ°å¯¹åº”çš„ sysarmor-audit-* topic
"""

import os
import json
import logging
import re
from typing import Dict, List, Optional, Any, Iterable, Tuple
from datetime import datetime, timedelta
from pyflink.datastream import StreamExecutionEnvironment
from pyflink.datastream.connectors.kafka import FlinkKafkaConsumer, FlinkKafkaProducer
from pyflink.common.serialization import SimpleStringSchema
from pyflink.common.typeinfo import Types
from pyflink.datastream.functions import (
    KeyedProcessFunction, ProcessWindowFunction, MapFunction, FilterFunction
)
from pyflink.datastream.state import (
    ValueStateDescriptor, MapStateDescriptor, ListStateDescriptor
)
from pyflink.datastream.window import TumblingProcessingTimeWindows
from pyflink.common.time import Time
from pyflink.common import Configuration

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class TopicEvent:
    """å¸¦ topic ä¿¡æ¯çš„äº‹ä»¶åŒ…è£…å™¨"""
    def __init__(self, topic: str, collector_id: str, event_data: Dict[str, Any]):
        self.topic = topic
        self.collector_id = collector_id
        self.event_data = event_data
        self.host = event_data.get('host', collector_id)
    
    def to_dict(self):
        return {
            'topic': self.topic,
            'collector_id': self.collector_id,
            'host': self.host,
            'event': self.event_data
        }

class ProcessInfo:
    """è¿›ç¨‹ä¿¡æ¯æ•°æ®ç»“æ„"""
    def __init__(self, pid: int, cmdline: str, timestamp: float, ppid: int = None, topic: str = None):
        self.pid = pid
        self.cmdline = cmdline
        self.timestamp = timestamp
        self.ppid = ppid
        self.topic = topic
    
    def to_dict(self):
        return {
            'pid': self.pid,
            'cmdline': self.cmdline,
            'timestamp': self.timestamp,
            'ppid': self.ppid,
            'topic': self.topic
        }

class TopicEventExtractor(MapFunction):
    """ä» Kafka æ¶ˆæ¯ä¸­æå– topic å’Œäº‹ä»¶ä¿¡æ¯"""
    
    def map(self, value):
        try:
            # Kafka æ¶ˆæ¯æ ¼å¼ï¼š(topic, message)
            if isinstance(value, tuple) and len(value) == 2:
                topic, message = value
            else:
                # å¦‚æœä¸æ˜¯ tupleï¼Œå°è¯•ä»æ¶ˆæ¯ä¸­æ¨æ–­
                message = value
                topic = "unknown"
            
            # è§£ææ¶ˆæ¯
            if isinstance(message, str):
                event_data = json.loads(message)
            else:
                event_data = message
            
            # ä» topic åç§°æå– collector_id
            collector_id = self._extract_collector_id(topic)
            
            # åˆ›å»º TopicEvent
            topic_event = TopicEvent(topic, collector_id, event_data)
            
            return json.dumps(topic_event.to_dict(), ensure_ascii=False)
            
        except Exception as e:
            logger.warning(f"Failed to extract topic event: {e}")
            return json.dumps({
                'topic': 'unknown',
                'collector_id': 'unknown',
                'host': 'unknown',
                'event': {'error': str(e)}
            })
    
    def _extract_collector_id(self, topic: str) -> str:
        """ä» topic åç§°æå– collector_id"""
        # sysarmor-agentless-558c01dd -> 558c01dd
        match = re.match(r'sysarmor-agentless-([a-f0-9]+)', topic)
        if match:
            return match.group(1)
        return 'unknown'

class MultiTopicKeySelector(MapFunction):
    """å¤š topic é”®é€‰æ‹©å™¨ï¼Œç¡®ä¿åŒä¸€ collector çš„äº‹ä»¶åœ¨åŒä¸€åˆ†åŒº"""
    
    def map(self, value):
        try:
            topic_event_data = json.loads(value) if isinstance(value, str) else value
            
            # ä½¿ç”¨ collector_id ä½œä¸ºé”®ï¼Œç¡®ä¿åŒä¸€é‡‡é›†å™¨çš„äº‹ä»¶åœ¨åŒä¸€åˆ†åŒº
            collector_id = topic_event_data.get('collector_id', 'unknown')
            
            return (collector_id, value)
            
        except Exception as e:
            logger.warning(f"Failed to select key: {e}")
            return ('unknown', value)

class MultiTopicProcessTreeBuilder(ProcessWindowFunction):
    """
    å¤š topic è¿›ç¨‹æ ‘æ„å»ºå™¨
    ä¸ºæ¯ä¸ª collector_id ç‹¬ç«‹ç»´æŠ¤è¿›ç¨‹æ ‘
    """
    
    def __init__(self, window_size_seconds: int = 300):
        self.window_size = window_size_seconds
        self.system_processes = {
            1: 'systemd --system --deserialize',
            2: '[kthreadd]',
            0: '[kernel]'
        }
        # æŒ‰ collector_id åˆ†åˆ«ç»Ÿè®¡
        self.stats_by_collector = {}
    
    def process(self, key, context, elements: Iterable, out) -> None:
        """å¤„ç†çª—å£å†…çš„æ‰€æœ‰äº‹ä»¶"""
        collector_id = key
        elements_list = list(elements)
        
        if not elements_list:
            return
        
        logger.info(f"Processing window for collector {collector_id} with {len(elements_list)} events")
        
        # æŒ‰ topic åˆ†ç»„äº‹ä»¶
        events_by_topic = self._group_events_by_topic(elements_list)
        
        # ä¸ºæ¯ä¸ª topic ç‹¬ç«‹å¤„ç†
        for topic, events in events_by_topic.items():
            try:
                self._process_topic_events(collector_id, topic, events, out)
            except Exception as e:
                logger.error(f"Error processing topic {topic} for collector {collector_id}: {e}")
                continue
        
        # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
        self._log_statistics(collector_id, len(elements_list))
    
    def _group_events_by_topic(self, elements_list: List[str]) -> Dict[str, List[Dict[str, Any]]]:
        """æŒ‰ topic åˆ†ç»„äº‹ä»¶"""
        events_by_topic = {}
        
        for element in elements_list:
            try:
                topic_event_data = json.loads(element)
                topic = topic_event_data.get('topic', 'unknown')
                event = topic_event_data.get('event', {})
                
                if topic not in events_by_topic:
                    events_by_topic[topic] = []
                
                events_by_topic[topic].append(event)
                
            except Exception as e:
                logger.warning(f"Failed to parse element: {e}")
                continue
        
        return events_by_topic
    
    def _process_topic_events(self, collector_id: str, topic: str, events: List[Dict[str, Any]], out):
        """å¤„ç†å•ä¸ª topic çš„äº‹ä»¶"""
        if not events:
            return
        
        logger.debug(f"Processing {len(events)} events for topic {topic}")
        
        # ç¬¬ä¸€éï¼šæ„å»ºè¯¥ topic çš„è¿›ç¨‹ç¼“å­˜
        process_cache = self._build_process_cache(events, topic)
        
        # ç¬¬äºŒéï¼šé‡å»ºçˆ¶è¿›ç¨‹ä¿¡æ¯
        enhanced_events = self._rebuild_parent_cmdlines(events, process_cache, collector_id)
        
        # ç”Ÿæˆè¾“å‡º topic åç§°
        output_topic = self._generate_output_topic(topic)
        
        # ç›´æ¥è¾“å‡ºå¢å¼ºåçš„äº‹ä»¶ï¼Œæ·»åŠ  topic æ ‡è¯†ç”¨äºåç»­åˆ†æµ
        for event in enhanced_events:
            # æ·»åŠ å…ƒæ•°æ®ä¿¡æ¯
            event['_metadata'] = {
                'input_topic': topic,
                'output_topic': output_topic,
                'collector_id': collector_id,
                'processed_time': context.window().get_end()
            }
            
            # è¾“å‡ºæ ¼å¼ï¼š(output_topic, enhanced_event_json)
            output_data = (output_topic, json.dumps(event, ensure_ascii=False))
            out.collect(json.dumps(output_data))
    
    def _build_process_cache(self, events: List[Dict[str, Any]], topic: str) -> Dict[int, ProcessInfo]:
        """ç¬¬ä¸€éï¼šæ„å»ºè¿›ç¨‹ç¼“å­˜"""
        process_cache = {}
        
        for event in events:
            try:
                pid = event.get('proc.pid')
                cmdline = event.get('proc.cmdline', '')
                timestamp = event.get('evt.time', 0)
                ppid = event.get('proc.ppid')
                
                if pid and cmdline:
                    # å¦‚æœå·²å­˜åœ¨ï¼Œé€‰æ‹©æ—¶é—´æˆ³æ›´æ–°çš„
                    if pid not in process_cache or timestamp > process_cache[pid].timestamp:
                        process_cache[pid] = ProcessInfo(pid, cmdline, timestamp, ppid, topic)
                        
            except Exception as e:
                logger.warning(f"Error processing event for cache: {e}")
                continue
        
        return process_cache
    
    def _rebuild_parent_cmdlines(self, events: List[Dict[str, Any]], 
                               process_cache: Dict[int, ProcessInfo],
                               collector_id: str) -> List[Dict[str, Any]]:
        """ç¬¬äºŒéï¼šé‡å»ºçˆ¶è¿›ç¨‹å‘½ä»¤è¡Œ"""
        enhanced_events = []
        
        # åˆå§‹åŒ–ç»Ÿè®¡
        if collector_id not in self.stats_by_collector:
            self.stats_by_collector[collector_id] = {
                'total_events': 0,
                'with_ppid': 0,
                'reconstructed': 0,
                'system_mapped': 0,
                'failed': 0
            }
        
        stats = self.stats_by_collector[collector_id]
        
        for event in events:
            try:
                stats['total_events'] += 1
                
                # å¤åˆ¶äº‹ä»¶
                enhanced_event = event.copy()
                
                # è·å–çˆ¶è¿›ç¨‹ID
                ppid = event.get('proc.ppid')
                event_time = event.get('evt.time', 0)
                
                if ppid:
                    stats['with_ppid'] += 1
                    parent_cmdline = self._get_parent_cmdline(
                        ppid, event_time, process_cache, stats
                    )
                    enhanced_event['proc.pcmdline'] = parent_cmdline
                else:
                    enhanced_event['proc.pcmdline'] = ''
                
                # æ·»åŠ é‡å»ºç»Ÿè®¡ä¿¡æ¯
                enhanced_event['_rebuild_stats'] = {
                    'collector_id': collector_id,
                    'cache_size': len(process_cache),
                    'window_events': len(events)
                }
                
                enhanced_events.append(enhanced_event)
                
            except Exception as e:
                logger.warning(f"Error rebuilding parent cmdline: {e}")
                enhanced_events.append(event)  # ä¿ç•™åŸäº‹ä»¶
                continue
        
        return enhanced_events
    
    def _get_parent_cmdline(self, ppid: int, event_time: float, 
                          process_cache: Dict[int, ProcessInfo],
                          stats: Dict[str, int]) -> str:
        """è·å–çˆ¶è¿›ç¨‹å‘½ä»¤è¡Œ"""
        
        # 1. ç³»ç»Ÿè¿›ç¨‹æ˜ å°„
        if ppid in self.system_processes:
            stats['system_mapped'] += 1
            return self.system_processes[ppid]
        
        # 2. ä»çª—å£ç¼“å­˜ä¸­æŸ¥æ‰¾
        if ppid in process_cache:
            parent_info = process_cache[ppid]
            time_diff = abs(parent_info.timestamp - event_time)
            if time_diff <= self.window_size:
                stats['reconstructed'] += 1
                return parent_info.cmdline
        
        # 3. æ‰©å±•æŸ¥æ‰¾
        best_match = None
        min_time_diff = float('inf')
        
        for cached_pid, cached_info in process_cache.items():
            if cached_pid == ppid:
                continue
            
            time_diff = abs(cached_info.timestamp - event_time)
            if time_diff < min_time_diff and time_diff <= self.window_size * 2:
                min_time_diff = time_diff
                best_match = cached_info
        
        if best_match:
            stats['reconstructed'] += 1
            return best_match.cmdline
        
        # 4. æ— æ³•é‡å»º
        stats['failed'] += 1
        return ''
    
    def _generate_output_topic(self, input_topic: str) -> str:
        """ç”Ÿæˆè¾“å‡º topic åç§°"""
        # sysarmor-agentless-558c01dd -> sysarmor-audit-558c01dd
        return input_topic.replace('sysarmor-agentless-', 'sysarmor-audit-')
    
    def _log_statistics(self, collector_id: str, window_events: int):
        """è®°å½•ç»Ÿè®¡ä¿¡æ¯"""
        if collector_id not in self.stats_by_collector:
            return
        
        stats = self.stats_by_collector[collector_id]
        total = stats['with_ppid']
        
        if total > 0:
            success_rate = (stats['reconstructed'] + stats['system_mapped']) / total * 100
            logger.info(f"Collector {collector_id} reconstruction stats:")
            logger.info(f"  Window events: {window_events}")
            logger.info(f"  Events with PPID: {stats['with_ppid']}")
            logger.info(f"  Successfully reconstructed: {stats['reconstructed']}")
            logger.info(f"  System process mapped: {stats['system_mapped']}")
            logger.info(f"  Failed: {stats['failed']}")
            logger.info(f"  Success rate: {success_rate:.1f}%")

class DirectOutputSink(MapFunction):
    """ç›´æ¥è¾“å‡ºåˆ°å¯¹åº” topic çš„ Sink"""
    
    def map(self, value):
        try:
            data = json.loads(value) if isinstance(value, str) else value
            
            # ç›´æ¥è¿”å›å¢å¼ºåçš„äº‹ä»¶ï¼Œä¸éœ€è¦è·¯ç”±ä¿¡æ¯
            message = data.get('message', '{}')
            return message
            
        except Exception as e:
            logger.error(f"Error in direct output: {e}")
            return json.dumps({'error': str(e)})

def discover_agentless_topics(kafka_servers: str) -> List[str]:
    """å‘ç°æ‰€æœ‰ sysarmor-agentless-* topics"""
    try:
        # è¿™é‡Œå¯ä»¥ä½¿ç”¨ Kafka Admin API æ¥åŠ¨æ€å‘ç° topics
        # ä¸ºäº†ç®€åŒ–ï¼Œå…ˆä½¿ç”¨ç¯å¢ƒå˜é‡æˆ–é…ç½®æ–‡ä»¶
        topics_pattern = os.getenv('AGENTLESS_TOPICS_PATTERN', 'sysarmor-agentless-.*')
        
        # å¦‚æœæœ‰å…·ä½“çš„ topic åˆ—è¡¨ï¼Œå¯ä»¥ä»ç¯å¢ƒå˜é‡è¯»å–
        topics_list = os.getenv('AGENTLESS_TOPICS_LIST', '')
        if topics_list:
            return [t.strip() for t in topics_list.split(',') if t.strip()]
        
        # å¦åˆ™è¿”å›é»˜è®¤çš„æµ‹è¯• topics
        return [
            'sysarmor-agentless-b1de298c'
        ]
        
    except Exception as e:
        logger.error(f"Failed to discover topics: {e}")
        return ['sysarmor-agentless-default']

def create_multi_topic_pipeline(env, kafka_props: Dict[str, str]):
    """åˆ›å»ºå¤š topic å¤„ç†ç®¡é“"""
    
    kafka_servers = kafka_props['bootstrap.servers']
    
    # 1. å‘ç°æ‰€æœ‰ agentless topics
    agentless_topics = discover_agentless_topics(kafka_servers)
    logger.info(f"Discovered {len(agentless_topics)} agentless topics: {agentless_topics[:5]}...")
    
    # 2. åˆ›å»ºå¤š topic Kafka Consumer
    kafka_consumer = FlinkKafkaConsumer(
        topics=agentless_topics,
        deserialization_schema=SimpleStringSchema(),
        properties=kafka_props
    )
    
    # 3. æ„å»ºå¤„ç†ç®¡é“
    processed_stream = env.add_source(kafka_consumer) \
        .map(TopicEventExtractor(), output_type=Types.STRING()) \
        .map(MultiTopicKeySelector(), output_type=Types.TUPLE([Types.STRING(), Types.STRING()])) \
        .key_by(lambda x: x[0]) \
        .window(TumblingProcessingTimeWindows.of(Time.minutes(5))) \
        .process(MultiTopicProcessTreeBuilder(), output_type=Types.STRING())
    
    # 4. è§£æè¾“å‡ºæ•°æ®ï¼Œæå– topic å’Œæ¶ˆæ¯
    def parse_output(value):
        try:
            data = json.loads(value)
            # data æ˜¯ (output_topic, enhanced_event_json) çš„ JSON
            if isinstance(data, list) and len(data) == 2:
                return data  # (topic, message)
            else:
                return ('sysarmor-audit-error', json.dumps({'error': 'Invalid output format'}))
        except Exception as e:
            return ('sysarmor-audit-error', json.dumps({'error': str(e)}))
    
    routed_stream = processed_stream.map(
        parse_output,
        output_type=Types.TUPLE([Types.STRING(), Types.STRING()])
    )
    
    # 5. æŒ‰ topic åˆ†æµå¹¶è¾“å‡ºåˆ°å¯¹åº”çš„ Kafka topics
    # ç”±äº Flink é™åˆ¶ï¼Œæˆ‘ä»¬è¾“å‡ºåˆ°ä¸€ä¸ªç»Ÿä¸€ topicï¼Œä½†æ¶ˆæ¯ä¸­åŒ…å«ç›®æ ‡ topic ä¿¡æ¯
    # è¿™æ ·ä¸‹æ¸¸å¯ä»¥æ ¹æ®æ¶ˆæ¯å†…å®¹è¿›è¡ŒäºŒæ¬¡åˆ†å‘
    unified_output_topic = 'sysarmor-audit-unified'
    
    kafka_producer = FlinkKafkaProducer(
        topic=unified_output_topic,
        serialization_schema=SimpleStringSchema(),
        producer_config=kafka_props
    )
    
    # å°† (topic, message) è½¬æ¢ä¸ºåŒ…å«è·¯ç”±ä¿¡æ¯çš„ç»Ÿä¸€æ ¼å¼
    final_stream = routed_stream.map(
        lambda x: json.dumps({
            'target_topic': x[0],  # ç›®æ ‡ topic: sysarmor-audit-558c01dd
            'message': x[1],       # å¢å¼ºåçš„äº‹ä»¶æ•°æ®
            'timestamp': int(datetime.now().timestamp() * 1000)
        }),
        output_type=Types.STRING()
    )
    
    final_stream.add_sink(kafka_producer)
    
    return final_stream

def main():
    """ä¸»å‡½æ•°"""
    logger.info("ğŸš€ Starting Multi-Topic Process Tree Builder Job")
    
    # ç¯å¢ƒé…ç½®
    kafka_servers = os.getenv('KAFKA_BOOTSTRAP_SERVERS', 'middleware-kafka:9092')
    
    logger.info(f"ğŸ“¡ Kafka Servers: {kafka_servers}")
    logger.info(f"ğŸ“¥ Input Pattern: sysarmor-agentless-*")
    logger.info(f"ğŸ“¤ Output Pattern: sysarmor-audit-*")
    
    # åˆ›å»ºæ‰§è¡Œç¯å¢ƒ
    env = StreamExecutionEnvironment.get_execution_environment()
    env.set_parallelism(5)  # å¢åŠ å¹¶è¡Œåº¦ä»¥å¤„ç†æ›´å¤š topics
    env.enable_checkpointing(30000)  # 30ç§’checkpoint
    
    # æ·»åŠ ä¾èµ–
    env.add_jars("file:///opt/flink/lib/flink-sql-connector-kafka-3.1.0-1.18.jar")
    
    # Kafkaé…ç½®
    kafka_props = {
        'bootstrap.servers': kafka_servers,
        'group.id': 'sysarmor-multi-topic-process-tree-group',
        'auto.offset.reset': 'latest',
        'max.poll.records': '1000',  # å¢åŠ æ‰¹é‡å¤§å°
        'fetch.max.wait.ms': '500'   # å‡å°‘ç­‰å¾…æ—¶é—´
    }
    
    # åˆ›å»ºå¤„ç†ç®¡é“
    pipeline = create_multi_topic_pipeline(env, kafka_props)
    
    logger.info("ğŸ”„ Multi-Topic Process Tree Pipeline created:")
    logger.info("   - Support 100-1000 topics simultaneously")
    logger.info("   - Independent processing per collector")
    logger.info("   - Two-pass window-based reconstruction")
    logger.info("   - Dynamic topic routing")
    logger.info("   - Unified output with routing metadata")
    
    # æ‰§è¡Œä½œä¸š
    job_client = env.execute_async("SysArmor-Multi-Topic-Process-Tree-Builder")
    job_id = job_client.get_job_id()
    
    logger.info(f"âœ… Multi-Topic Process Tree job started!")
    logger.info(f"ğŸ“‹ Job ID: {job_id}")
    logger.info(f"ğŸŒ Monitor at: http://localhost:8081")
    
    return job_id

if __name__ == "__main__":
    main()
