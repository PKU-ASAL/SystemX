#!/usr/bin/env python3
"""
SysArmor Processor - Events to Alerts Job
æ¶ˆè´¹ sysarmor.events.audit topicï¼ŒåŸºäºå¨èƒæ£€æµ‹è§„åˆ™è¿‡æ»¤å‡ºå‘Šè­¦äº‹ä»¶
è¾“å‡ºåˆ° sysarmor.alerts å’Œ sysarmor.alerts.high topics
åŸºäº Falco/Sysdig è§„åˆ™å¼•æ“è®¾è®¡
"""

import os
import json
import logging
import re
import yaml
import requests
import uuid
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any
from collections import defaultdict, deque
from pyflink.datastream import StreamExecutionEnvironment, CheckpointingMode
from pyflink.datastream.connectors.kafka import FlinkKafkaConsumer, FlinkKafkaProducer
# ç§»é™¤ä¸å…¼å®¹çš„ ElasticsearchSink å¯¼å…¥
from pyflink.common.serialization import SimpleStringSchema
from pyflink.common.typeinfo import Types
from pyflink.datastream.functions import MapFunction, FilterFunction
from pyflink.datastream.state import ValueStateDescriptor
from pyflink.datastream.functions import KeyedProcessFunction, ProcessFunction
from pyflink.common import Time

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class ThreatDetectionRules:
    """å¨èƒæ£€æµ‹è§„åˆ™å¼•æ“ - åŸºäº Falco è§„åˆ™è®¾è®¡"""
    
    def __init__(self, rules_file: str = "/opt/flink/configs/rules/threat_detection_rules.yaml"):
        self.rules = {}
        self.rule_groups = {}
        self.global_settings = {}
        self.load_rules(rules_file)
        
    def load_rules(self, rules_file: str):
        """åŠ è½½å¨èƒæ£€æµ‹è§„åˆ™"""
        try:
            if os.path.exists(rules_file):
                with open(rules_file, 'r', encoding='utf-8') as f:
                    config = yaml.safe_load(f)
                
                # åŠ è½½è§„åˆ™
                for rule in config.get('rules', []):
                    if rule.get('enabled', True):
                        self.rules[rule['id']] = rule
                
                # åŠ è½½è§„åˆ™ç»„
                self.rule_groups = config.get('rule_groups', {})
                
                # åŠ è½½å…¨å±€è®¾ç½®
                self.global_settings = config.get('global_settings', {})
                
                logger.info(f"âœ… åŠ è½½äº† {len(self.rules)} ä¸ªå¨èƒæ£€æµ‹è§„åˆ™")
                logger.info(f"ğŸ“‹ è§„åˆ™ç»„: {list(self.rule_groups.keys())}")
            else:
                logger.warning(f"è§„åˆ™æ–‡ä»¶ä¸å­˜åœ¨: {rules_file}ï¼Œä½¿ç”¨é»˜è®¤è§„åˆ™")
                self._load_default_rules()
                
        except Exception as e:
            logger.error(f"åŠ è½½è§„åˆ™å¤±è´¥: {e}ï¼Œä½¿ç”¨é»˜è®¤è§„åˆ™")
            self._load_default_rules()
    
    def _load_default_rules(self):
        """åŠ è½½é»˜è®¤è§„åˆ™"""
        self.rules = {
            "suspicious_tmp_execution": {
                "id": "suspicious_tmp_execution",
                "name": "å¯ç–‘ä¸´æ—¶ç›®å½•ç¨‹åºæ‰§è¡Œ",
                "category": "suspicious_activity",
                "severity": "high",
                "base_score": 85,
                "patterns": [r'proc\.exe.*"/tmp/', r'proc\.exe.*"/dev/shm/'],
                "frequency_threshold": 1,
                "time_window": 300
            },
            "privilege_escalation_setuid": {
                "id": "privilege_escalation_setuid",
                "name": "SetUIDæƒé™æå‡",
                "category": "privilege_escalation", 
                "severity": "critical",
                "base_score": 90,
                "patterns": [r'evt\.type.*setuid', r'evt\.type.*setgid'],
                "frequency_threshold": 1,
                "time_window": 300
            }
        }
        logger.info("âœ… åŠ è½½äº†é»˜è®¤å¨èƒæ£€æµ‹è§„åˆ™")
    
    def evaluate_event(self, event: Dict[str, Any]) -> List[Dict[str, Any]]:
        """è¯„ä¼°äº‹ä»¶æ˜¯å¦è§¦å‘å¨èƒæ£€æµ‹è§„åˆ™"""
        alerts = []
        
        # è·å–äº‹ä»¶çš„ sysdig æ•°æ®
        sysdig_data = event.get('message', {})
        event_str = json.dumps(event, ensure_ascii=False)
        
        for rule_id, rule in self.rules.items():
            if self._match_rule(event, sysdig_data, event_str, rule):
                alert = self._create_alert(event, rule)
                alerts.append(alert)
        
        return alerts
    
    def _match_rule(self, event: Dict, sysdig_data: Dict, event_str: str, rule: Dict) -> bool:
        """æ£€æŸ¥äº‹ä»¶æ˜¯å¦åŒ¹é…è§„åˆ™"""
        try:
            # æ£€æŸ¥å…³é”®è¯åŒ¹é…
            keywords = rule.get('keywords', [])
            for keyword in keywords:
                if keyword in event_str:
                    return True
            
            # æ£€æŸ¥æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…
            patterns = rule.get('patterns', [])
            for pattern in patterns:
                if re.search(pattern, event_str, re.IGNORECASE):
                    return True
            
            # æ£€æŸ¥å­—æ®µæ¡ä»¶åŒ¹é…
            conditions = rule.get('conditions', {})
            if conditions:
                # ç®€å•çš„å­—æ®µåŒ¹é…é€»è¾‘
                for field, expected_value in conditions.items():
                    if field in event and event[field] == expected_value:
                        return True
                    if field in sysdig_data and sysdig_data[field] == expected_value:
                        return True
            
            return False
            
        except Exception as e:
            logger.debug(f"è§„åˆ™åŒ¹é…å¼‚å¸¸ {rule_id}: {e}")
            return False
    
    def _create_alert(self, event: Dict, rule: Dict) -> Dict[str, Any]:
        """åˆ›å»ºå‘Šè­¦äº‹ä»¶"""
        now = datetime.utcnow()
        
        # è®¡ç®—é£é™©è¯„åˆ†
        base_score = rule.get('base_score', 50)
        score_multiplier = rule.get('score_multiplier', 1.0)
        final_score = min(100, int(base_score * score_multiplier))
        
        # ç¡®å®šä¸¥é‡ç¨‹åº¦
        severity = rule.get('severity', 'medium')
        if final_score >= 90:
            severity = 'critical'
        elif final_score >= 70:
            severity = 'high'
        elif final_score >= 50:
            severity = 'medium'
        else:
            severity = 'low'
        
        alert = {
            # OpenSearch æ ‡å‡†ä¸»æ—¶é—´å­—æ®µ
            "@timestamp": now.isoformat() + 'Z',
            
            # å‘Šè­¦æ ¸å¿ƒä¿¡æ¯
            "alert": {
                "id": str(uuid.uuid4()),
                "type": "rule_based_detection",
                "category": rule.get('category', 'unknown'),
                "severity": severity,
                "risk_score": final_score,
                "confidence": 0.8,
                "rule": {
                    "id": rule['id'],
                    "name": rule.get('name', ''),
                    "description": rule.get('description', ''),
                    "title": f"{rule.get('name', 'Unknown Threat')}: {event.get('event_type', 'unknown')}",
                    "mitigation": f"æ£€æŸ¥ {rule.get('category', 'unknown')} ç›¸å…³æ´»åŠ¨",
                    "references": [f"SysArmor Rule: {rule['id']}"]
                },
                "evidence": {
                    "event_type": event.get('event_type', ''),
                    "process_name": event.get('message', {}).get('proc.name', ''),
                    "process_cmdline": event.get('message', {}).get('proc.cmdline', ''),
                    "file_path": event.get('message', {}).get('fd.name', ''),
                    "network_info": event.get('message', {}).get('net.sockaddr', {})
                }
            },
            
            # åŸå§‹äº‹ä»¶æ•°æ®
            "event": {
                "raw": {
                    "event_id": event.get('event_id', ''),
                    "timestamp": event.get('timestamp', ''),
                    "source": event.get('source', 'auditd'),
                    "message": event.get('message', {})  # å®Œæ•´çš„ sysdig æ•°æ®ï¼ŒåŒ…å« evt.time
                }
            },
            
            # æ—¶é—´ä¿¡æ¯
            "timing": {
                "created_at": now.isoformat() + 'Z',
                "processed_at": now.isoformat() + 'Z'
            },
            
            # å…ƒæ•°æ®ä¿¡æ¯
            "metadata": {
                "collector_id": event.get('collector_id', ''),
                "host": event.get('host', 'unknown'),
                "source": "sysarmor-threat-detector",
                "processor": "flink-events-to-alerts"
            }
        }
        
        return alert

class EventToAlertsProcessor(MapFunction):
    """äº‹ä»¶åˆ°å‘Šè­¦å¤„ç†å™¨ - ç®€åŒ–ç‰ˆæœ¬ï¼Œå…ˆæµ‹è¯•åŸºæœ¬åŠŸèƒ½"""
    
    def __init__(self):
        self.rules_engine = ThreatDetectionRules()
        
    def map(self, value):
        try:
            event = json.loads(value)
            logger.info(f"ğŸ” å¤„ç†äº‹ä»¶: {event.get('event_type', 'unknown')} from {event.get('collector_id', 'unknown')[:8]}")
            
            # åŸºç¡€è§„åˆ™åŒ¹é…
            alerts = self.rules_engine.evaluate_event(event)
            
            if alerts:
                logger.info(f"ğŸš¨ åŒ¹é…åˆ° {len(alerts)} ä¸ªå‘Šè­¦è§„åˆ™")
                # è¿”å›ç¬¬ä¸€ä¸ªåŒ¹é…çš„å‘Šè­¦
                alert = alerts[0]
                logger.info(f"ğŸš¨ ç”Ÿæˆå‘Šè­¦: {alert['alert']['id']} - {alert['alert']['rule']['name']}")
                return json.dumps(alert, ensure_ascii=False)
            
            return None
                
        except Exception as e:
            logger.error(f"å¤„ç†äº‹ä»¶å¼‚å¸¸: {e}")
            return None

class AlertSeverityRouter(FilterFunction):
    """å‘Šè­¦ä¸¥é‡ç¨‹åº¦è·¯ç”±å™¨"""
    
    def __init__(self, target_severity: str = "high"):
        self.target_severity = target_severity
    
    def filter(self, value):
        try:
            alert = json.loads(value)
            severity = alert.get('alert', {}).get('severity', 'low')
            
            if self.target_severity == "high":
                return severity in ['high', 'critical']
            else:
                return severity in ['low', 'medium']
                
        except Exception:
            return False

def main():
    """ä¸»å‡½æ•°ï¼šåˆ›å»ºäº‹ä»¶åˆ°å‘Šè­¦çš„å¤„ç†ä½œä¸š"""
    
    logger.info("ğŸš€ Starting SysArmor Audit Events to Alerts Job")
    logger.info("ğŸ“‹ Based on Falco-style rule engine")
    logger.info("ğŸ“Š Processing: sysarmor.events.audit â†’ sysarmor.alerts.audit")
    
    # ç¯å¢ƒå˜é‡é…ç½®
    kafka_servers = os.getenv('KAFKA_BOOTSTRAP_SERVERS', 'middleware-kafka:9092')
    input_topic = 'sysarmor.events.audit'
    output_topic = 'sysarmor.alerts.audit'  # ç®€åŒ–ä¸ºå•ä¸€å‘Šè­¦topic
    kafka_group_id = 'sysarmor-audit-events-to-alerts-processor'  # æ›´æ–°Consumer Groupåç§°
    
    logger.info(f"ğŸ“¡ Kafka Servers: {kafka_servers}")
    logger.info(f"ğŸ“¥ Input Topic: {input_topic}")
    logger.info(f"ğŸ“¤ Output Topic: {output_topic}")
    logger.info(f"ğŸ‘¥ Consumer Group: {kafka_group_id}")
    
    # åˆ›å»ºæµå¤„ç†ç¯å¢ƒ
    env = StreamExecutionEnvironment.get_execution_environment()
    
    # é…ç½®ç¯å¢ƒ
    env.set_parallelism(2)  # 2ä¸ªå¹¶è¡Œåº¦
    env.enable_checkpointing(30000)  # 30ç§’ checkpoint
    env.get_checkpoint_config().set_checkpointing_mode(CheckpointingMode.EXACTLY_ONCE)
    
    try:
        # æ·»åŠ  JAR ä¾èµ–
        env.add_jars("file:///opt/flink/lib/flink-sql-connector-kafka-3.1.0-1.18.jar")
        
        # åˆ›å»º Kafka Consumer
        consumer_props = {
            'bootstrap.servers': kafka_servers,
            'group.id': kafka_group_id,
            'auto.offset.reset': 'earliest',  # å¤„ç†æ‰€æœ‰äº‹ä»¶ï¼ŒåŒ…æ‹¬å†å²äº‹ä»¶
            'session.timeout.ms': '30000',
            'heartbeat.interval.ms': '10000',
            'max.poll.interval.ms': '300000'
        }
        
        kafka_consumer = FlinkKafkaConsumer(
            topics=[input_topic],
            deserialization_schema=SimpleStringSchema(),
            properties=consumer_props
        )
        
        # åˆ›å»º Kafka Producer (ç®€åŒ–ä¸ºå•ä¸€å‘Šè­¦æµ)
        producer_props = {
            'bootstrap.servers': kafka_servers,
            'transaction.timeout.ms': '900000',
            'batch.size': '16384',
            'linger.ms': '5',
            'compression.type': 'snappy'
        }
        
        kafka_producer = FlinkKafkaProducer(
            topic=output_topic,
            serialization_schema=SimpleStringSchema(),
            producer_config=producer_props
        )
        
        logger.info("ğŸ“‹ Creating Falco-style threat detection pipeline...")
        
        # æ„å»ºæ•°æ®æµå¤„ç†ç®¡é“
        events_stream = env.add_source(kafka_consumer)
        
        # æŒ‰ collector_id åˆ†ç»„ï¼Œæ”¯æŒé¢‘ç‡æ£€æµ‹
        keyed_stream = events_stream.key_by(
            lambda event: json.loads(event).get('collector_id', 'unknown')
        )
        
        # å¨èƒæ£€æµ‹å¤„ç† (ç®€åŒ–ç‰ˆæœ¬)
        alerts_stream = events_stream.map(
            EventToAlertsProcessor(),
            output_type=Types.STRING()
        ).filter(lambda x: x is not None)
        
        # é…ç½® OpenSearch sink
        opensearch_url = os.getenv('OPENSEARCH_URL', 'http://opensearch:9200')
        opensearch_username = os.getenv('OPENSEARCH_USERNAME', 'admin')
        opensearch_password = os.getenv('OPENSEARCH_PASSWORD', 'admin')
        
        logger.info(f"ğŸ” OpenSearch URL: {opensearch_url}")
        
        # åˆ›å»º OpenSearch HTTP Sink (å€Ÿé‰´å·¥ä½œç‰ˆæœ¬çš„æ–¹æ¡ˆ)
        class OpenSearchHttpSink(MapFunction):
            """OpenSearch HTTP Sink - ä½¿ç”¨ HTTP è¯·æ±‚å†™å…¥"""
            
            def __init__(self):
                self.opensearch_url = opensearch_url
                self.opensearch_username = opensearch_username
                self.opensearch_password = opensearch_password
                self.index_url = f"{opensearch_url}/sysarmor-alerts-audit/_doc"
                
            def map(self, value):
                try:
                    if not value:
                        return value
                    
                    alert_data = json.loads(value)
                    
                    headers = {'Content-Type': 'application/json'}
                    auth = (self.opensearch_username, self.opensearch_password)
                    
                    response = requests.post(
                        self.index_url,
                        json=alert_data,
                        headers=headers,
                        auth=auth,
                        timeout=10,
                        verify=False
                    )
                    
                    if response.status_code in [200, 201]:
                        logger.info(f"âœ… å‘Šè­¦å†™å…¥ OpenSearch: {alert_data.get('alert', {}).get('id', 'unknown')}")
                    else:
                        logger.error(f"âŒ OpenSearch å†™å…¥å¤±è´¥: {response.status_code}")
                        
                except Exception as e:
                    logger.error(f"âŒ OpenSearch HTTP Sink é”™è¯¯: {e}")
                
                return value
        
        opensearch_http_sink = OpenSearchHttpSink()
        logger.info("âœ… OpenSearch HTTP sink å·²é…ç½®: sysarmor-alerts-audit")
        
        # ç®€åŒ–çš„å‘Šè­¦è¾“å‡º (å•ä¸€å‘Šè­¦æµ)
        alerts_stream.add_sink(kafka_producer)
        
        # æ‰€æœ‰å‘Šè­¦å†™å…¥ OpenSearch (ä½¿ç”¨ HTTP æ–¹å¼)
        alerts_stream.map(opensearch_http_sink, output_type=Types.STRING())
        logger.info("âœ… æ‰€æœ‰å‘Šè­¦å°†å†™å…¥ OpenSearch: sysarmor-alerts-audit")
        
        logger.info("âœ… å‘Šè­¦å°†å†™å…¥ Kafka Topic + OpenSearch")
        
        # ç›‘æ§è¾“å‡º
        alerts_stream.map(
            lambda x: f"ğŸš¨ Alert: {json.loads(x).get('alert', {}).get('severity', 'unknown')} - {json.loads(x).get('alert', {}).get('rule', {}).get('name', 'unknown')} from {json.loads(x).get('metadata', {}).get('collector_id', 'unknown')[:8]}",
            output_type=Types.STRING()
        ).print()
        
        logger.info("ğŸ”„ Falco-style threat detection pipeline created:")
        logger.info(f"   {input_topic} -> Rule Engine -> Threat Detection -> {output_topic}")
        
        # æ˜¾ç¤ºåŠ è½½çš„è§„åˆ™
        rules_engine = ThreatDetectionRules()
        logger.info("ğŸ›¡ï¸ åŠ è½½çš„å¨èƒæ£€æµ‹è§„åˆ™:")
        for rule_id, rule in rules_engine.rules.items():
            logger.info(f"   - {rule_id}: {rule.get('name', '')} ({rule.get('severity', 'unknown')})")
        
        logger.info("ğŸ¯ å‘Šè­¦è¾“å‡º:")
        logger.info(f"   - Kafka Topic: {output_topic}")
        logger.info(f"   - OpenSearchç´¢å¼•: sysarmor-alerts-audit")
        
        # æ‰§è¡Œä½œä¸š
        logger.info("âœ… Starting audit threat detection job...")
        
        job_client = env.execute_async("SysArmor-Audit-Events-to-Alerts-Processor")
        
        logger.info(f"ğŸ¯ Audit Events to Alerts job submitted successfully!")
        logger.info(f"ğŸ“‹ Job submitted with async execution")
        logger.info(f"ğŸŒ Monitor at: http://localhost:8081")
        logger.info(f"ğŸ“Š Processing: {input_topic} â†’ {output_topic}")
        logger.info(f"ğŸ” View logs: docker logs -f sysarmor-flink-taskmanager-1")
        
        return "async-job-submitted"
        
    except Exception as e:
        logger.error(f"âŒ Events to Alerts job failed: {e}")
        raise

if __name__ == "__main__":
    main()
