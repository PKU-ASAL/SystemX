#!/usr/bin/env python3
"""
SysArmor Processor - Events to Alerts Job
æ¶ˆè´¹ sysarmor.events.audit topicï¼ŒåŸºäºå¨èƒæ£€æµ‹è§„åˆ™è¿‡æ»¤å‡ºå‘Šè­¦äº‹ä»¶
è¾“å‡ºåˆ° sysarmor.alerts å’Œ sysarmor.alerts.high topics
åŸºäº Falco/Sysdig è§„åˆ™å¼•æ“è®¾è®¡
"""

import os
import json
import logging
import requests
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any
from collections import defaultdict, deque
from pyflink.datastream import StreamExecutionEnvironment, CheckpointingMode
from pyflink.datastream.connectors.kafka import FlinkKafkaConsumer, FlinkKafkaProducer
# ç§»é™¤ä¸å…¼å®¹çš„ ElasticsearchSink å¯¼å…¥
from pyflink.common.serialization import SimpleStringSchema
from pyflink.common.typeinfo import Types
from pyflink.datastream.functions import MapFunction, FilterFunction
from pyflink.datastream.state import ValueStateDescriptor
from pyflink.datastream.functions import KeyedProcessFunction, ProcessFunction
from pyflink.common import Time

# å¯¼å…¥å¨èƒæ£€æµ‹å¼•æ“ï¼ˆå¤ç”¨æ¨¡å—ï¼‰
from threat_detection_engine import ThreatDetectionRules

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class EventToAlertsProcessor(MapFunction):
    """äº‹ä»¶åˆ°å‘Šè­¦å¤„ç†å™¨ - ç®€åŒ–ç‰ˆæœ¬ï¼Œå…ˆæµ‹è¯•åŸºæœ¬åŠŸèƒ½"""
    
    def __init__(self):
        self.rules_engine = ThreatDetectionRules()
        
    def map(self, value):
        try:
            event = json.loads(value)
            logger.info(f"ğŸ” å¤„ç†äº‹ä»¶: {event.get('event_type', 'unknown')} from {event.get('collector_id', 'unknown')[:8]}")
            
            # åŸºç¡€è§„åˆ™åŒ¹é…
            alerts = self.rules_engine.evaluate_event(event)
            
            if alerts:
                logger.info(f"ğŸš¨ åŒ¹é…åˆ° {len(alerts)} ä¸ªå‘Šè­¦è§„åˆ™")
                # è¿”å›ç¬¬ä¸€ä¸ªåŒ¹é…çš„å‘Šè­¦
                alert = alerts[0]
                logger.info(f"ğŸš¨ ç”Ÿæˆå‘Šè­¦: {alert['alert']['id']} - {alert['alert']['rule']['name']}")
                return json.dumps(alert, ensure_ascii=False)
            
            return None
                
        except Exception as e:
            logger.error(f"å¤„ç†äº‹ä»¶å¼‚å¸¸: {e}")
            return None


class AlertSeverityRouter(FilterFunction):
    """å‘Šè­¦ä¸¥é‡ç¨‹åº¦è·¯ç”±å™¨"""
    
    def __init__(self, target_severity: str = "high"):
        self.target_severity = target_severity
    
    def filter(self, value):
        try:
            alert = json.loads(value)
            severity = alert.get('alert', {}).get('severity', 'low')
            
            if self.target_severity == "high":
                return severity in ['high', 'critical']
            else:
                return severity in ['low', 'medium']
                
        except Exception:
            return False


def main():
    """ä¸»å‡½æ•°ï¼šåˆ›å»ºäº‹ä»¶åˆ°å‘Šè­¦çš„å¤„ç†ä½œä¸š"""
    
    logger.info("ğŸš€ Starting SysArmor Audit Events to Alerts Job")
    logger.info("ğŸ“‹ Based on Falco-style rule engine")
    logger.info("ğŸ“Š Processing: sysarmor.events.audit â†’ sysarmor.alerts.audit")
    
    # ç¯å¢ƒå˜é‡é…ç½®
    kafka_servers = os.getenv('KAFKA_BOOTSTRAP_SERVERS', 'middleware-kafka:9092')
    input_topic = 'sysarmor.events.audit'
    output_topic = 'sysarmor.alerts.audit'  # ç®€åŒ–ä¸ºå•ä¸€å‘Šè­¦topic
    kafka_group_id = 'sysarmor-audit-events-to-alerts-processor'  # æ›´æ–°Consumer Groupåç§°
    
    logger.info(f"ğŸ“¡ Kafka Servers: {kafka_servers}")
    logger.info(f"ğŸ“¥ Input Topic: {input_topic}")
    logger.info(f"ğŸ“¤ Output Topic: {output_topic}")
    logger.info(f"ğŸ‘¥ Consumer Group: {kafka_group_id}")
    
    # åˆ›å»ºæµå¤„ç†ç¯å¢ƒ
    env = StreamExecutionEnvironment.get_execution_environment()
    
    # é…ç½®ç¯å¢ƒ
    env.set_parallelism(2)  # 2ä¸ªå¹¶è¡Œåº¦
    env.enable_checkpointing(30000)  # 30ç§’ checkpoint
    env.get_checkpoint_config().set_checkpointing_mode(CheckpointingMode.EXACTLY_ONCE)
    
    try:
        # æ·»åŠ  JAR ä¾èµ–
        env.add_jars("file:///opt/flink/lib/flink-sql-connector-kafka-3.1.0-1.18.jar")
        
        # åˆ›å»º Kafka Consumer
        consumer_props = {
            'bootstrap.servers': kafka_servers,
            'group.id': kafka_group_id,
            'auto.offset.reset': 'earliest',  # å¤„ç†æ‰€æœ‰äº‹ä»¶ï¼ŒåŒ…æ‹¬å†å²äº‹ä»¶
            'session.timeout.ms': '30000',
            'heartbeat.interval.ms': '10000',
            'max.poll.interval.ms': '300000'
        }
        
        kafka_consumer = FlinkKafkaConsumer(
            topics=[input_topic],
            deserialization_schema=SimpleStringSchema(),
            properties=consumer_props
        )
        
        # åˆ›å»º Kafka Producer (ç®€åŒ–ä¸ºå•ä¸€å‘Šè­¦æµ)
        producer_props = {
            'bootstrap.servers': kafka_servers,
            'transaction.timeout.ms': '900000',
            'batch.size': '16384',
            'linger.ms': '5',
            'compression.type': 'snappy'
        }
        
        kafka_producer = FlinkKafkaProducer(
            topic=output_topic,
            serialization_schema=SimpleStringSchema(),
            producer_config=producer_props
        )
        
        logger.info("ğŸ“‹ Creating Falco-style threat detection pipeline...")
        
        # æ„å»ºæ•°æ®æµå¤„ç†ç®¡é“
        events_stream = env.add_source(kafka_consumer)
        
        # æŒ‰ collector_id åˆ†ç»„ï¼Œæ”¯æŒé¢‘ç‡æ£€æµ‹
        keyed_stream = events_stream.key_by(
            lambda event: json.loads(event).get('collector_id', 'unknown')
        )
        
        # å¨èƒæ£€æµ‹å¤„ç† (ç®€åŒ–ç‰ˆæœ¬)
        alerts_stream = events_stream.map(
            EventToAlertsProcessor(),
            output_type=Types.STRING()
        ).filter(lambda x: x is not None)
        
        # é…ç½® OpenSearch sink
        opensearch_url = os.getenv('OPENSEARCH_URL', 'http://opensearch:9200')
        opensearch_username = os.getenv('OPENSEARCH_USERNAME', 'admin')
        opensearch_password = os.getenv('OPENSEARCH_PASSWORD', 'admin')
        
        logger.info(f"ğŸ” OpenSearch URL: {opensearch_url}")
        
        # åˆ›å»º OpenSearch HTTP Sink (å€Ÿé‰´å·¥ä½œç‰ˆæœ¬çš„æ–¹æ¡ˆ)
        class OpenSearchHttpSink(MapFunction):
            """OpenSearch HTTP Sink - ä½¿ç”¨ HTTP è¯·æ±‚å†™å…¥"""
            
            def __init__(self):
                self.opensearch_url = opensearch_url
                self.opensearch_username = opensearch_username
                self.opensearch_password = opensearch_password
                self.index_url = f"{opensearch_url}/sysarmor-alerts-audit/_doc"
                
            def map(self, value):
                try:
                    if not value:
                        return value
                    
                    alert_data = json.loads(value)
                    
                    headers = {'Content-Type': 'application/json'}
                    auth = (self.opensearch_username, self.opensearch_password)
                    
                    response = requests.post(
                        self.index_url,
                        json=alert_data,
                        headers=headers,
                        auth=auth,
                        timeout=10,
                        verify=False
                    )
                    
                    if response.status_code in [200, 201]:
                        logger.info(f"âœ… å‘Šè­¦å†™å…¥ OpenSearch: {alert_data.get('alert', {}).get('id', 'unknown')}")
                    else:
                        logger.error(f"âŒ OpenSearch å†™å…¥å¤±è´¥: {response.status_code}")
                        
                except Exception as e:
                    logger.error(f"âŒ OpenSearch HTTP Sink é”™è¯¯: {e}")
                
                return value
        
        opensearch_http_sink = OpenSearchHttpSink()
        logger.info("âœ… OpenSearch HTTP sink å·²é…ç½®: sysarmor-alerts-audit")
        
        # ç®€åŒ–çš„å‘Šè­¦è¾“å‡º (å•ä¸€å‘Šè­¦æµ)
        alerts_stream.add_sink(kafka_producer)
        
        # æ‰€æœ‰å‘Šè­¦å†™å…¥ OpenSearch (ä½¿ç”¨ HTTP æ–¹å¼)
        alerts_stream.map(opensearch_http_sink, output_type=Types.STRING())
        logger.info("âœ… æ‰€æœ‰å‘Šè­¦å°†å†™å…¥ OpenSearch: sysarmor-alerts-audit")
        
        logger.info("âœ… å‘Šè­¦å°†å†™å…¥ Kafka Topic + OpenSearch")
        
        # ç›‘æ§è¾“å‡º
        alerts_stream.map(
            lambda x: f"ğŸš¨ Alert: {json.loads(x).get('alert', {}).get('severity', 'unknown')} - {json.loads(x).get('alert', {}).get('rule', {}).get('name', 'unknown')} from {json.loads(x).get('metadata', {}).get('collector_id', 'unknown')[:8]}",
            output_type=Types.STRING()
        ).print()
        
        logger.info("ğŸ”„ Falco-style threat detection pipeline created:")
        logger.info(f"   {input_topic} -> Rule Engine -> Threat Detection -> {output_topic}")
        
        # æ˜¾ç¤ºåŠ è½½çš„è§„åˆ™
        rules_engine = ThreatDetectionRules()
        logger.info("ğŸ›¡ï¸ åŠ è½½çš„å¨èƒæ£€æµ‹è§„åˆ™:")
        for rule_id, rule in rules_engine.rules.items():
            logger.info(f"   - {rule_id}: {rule.get('name', '')} ({rule.get('severity', 'unknown')})")
        
        logger.info("ğŸ¯ å‘Šè­¦è¾“å‡º:")
        logger.info(f"   - Kafka Topic: {output_topic}")
        logger.info(f"   - OpenSearchç´¢å¼•: sysarmor-alerts-audit")
        
        # æ‰§è¡Œä½œä¸š
        logger.info("âœ… Starting audit threat detection job...")
        
        job_client = env.execute_async("SysArmor-Audit-Events-to-Alerts-Processor")
        
        logger.info(f"ğŸ¯ Audit Events to Alerts job submitted successfully!")
        logger.info(f"ğŸ“‹ Job submitted with async execution")
        logger.info(f"ğŸŒ Monitor at: http://localhost:8081")
        logger.info(f"ğŸ“Š Processing: {input_topic} â†’ {output_topic}")
        logger.info(f"ğŸ” View logs: docker logs -f sysarmor-flink-taskmanager-1")
        
        return "async-job-submitted"
        
    except Exception as e:
        logger.error(f"âŒ Events to Alerts job failed: {e}")
        raise


if __name__ == "__main__":
    main()
