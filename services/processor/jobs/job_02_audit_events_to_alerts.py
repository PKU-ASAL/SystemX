#!/usr/bin/env python3
"""
SysArmor Processor - Events to Alerts Job
æ¶ˆè´¹ sysarmor.events.audit topicï¼ŒåŸºäºå¨èƒæ£€æµ‹è§„åˆ™è¿‡æ»¤å‡ºå‘Šè­¦äº‹ä»¶
è¾“å‡ºåˆ°ä¸¤ä¸ª Kafka topics:
  1. sysarmor.alerts.audit - å‘Šè­¦äº‹ä»¶
  2. sysarmor.inference.requests - è§„èŒƒåŒ–çš„æ¨ç†è¯·æ±‚ï¼ˆä¾›job03æ¶ˆè´¹ï¼‰
åŸºäº Falco/Sysdig è§„åˆ™å¼•æ“è®¾è®¡
"""

import os
import json
import logging
import requests
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any
from collections import defaultdict, deque
from pyflink.datastream import StreamExecutionEnvironment, CheckpointingMode
from pyflink.datastream.connectors.kafka import FlinkKafkaConsumer, FlinkKafkaProducer
# ç§»é™¤ä¸å…¼å®¹çš„ ElasticsearchSink å¯¼å…¥
from pyflink.common.serialization import SimpleStringSchema
from pyflink.common.typeinfo import Types
from pyflink.datastream.functions import MapFunction, FilterFunction
from pyflink.datastream.state import ValueStateDescriptor
from pyflink.datastream.functions import KeyedProcessFunction, ProcessFunction
from pyflink.common import Time

# å¯¼å…¥å¨èƒæ£€æµ‹å¼•æ“ï¼ˆå¤ç”¨æ¨¡å—ï¼‰
from threat_detection_engine import ThreatDetectionRules

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# å…¨å±€å¨èƒæ£€æµ‹å¼•æ“å®ä¾‹
_global_rules_engine = None

def get_rules_engine():
    """è·å–å…¨å±€å¨èƒæ£€æµ‹å¼•æ“å®ä¾‹"""
    global _global_rules_engine
    if _global_rules_engine is None:
        threat_logger = logging.getLogger('threat_detection_engine')
        original_level = threat_logger.level
        threat_logger.setLevel(logging.WARNING)
        
        _global_rules_engine = ThreatDetectionRules()
        
        # æ¢å¤æ—¥å¿—çº§åˆ«
        threat_logger.setLevel(original_level)
        
        logger.info(f"âœ… å¨èƒæ£€æµ‹å¼•æ“å·²åˆå§‹åŒ–: {len(_global_rules_engine.rules)} ä¸ªè§„åˆ™")
    return _global_rules_engine


class EventNormalizer:
    """äº‹ä»¶è§„èŒƒåŒ–å™¨ - å°†åŸå§‹äº‹ä»¶è½¬æ¢ä¸ºæ¨ç†æœåŠ¡æ‰€éœ€æ ¼å¼"""
    
    @staticmethod
    def normalize_event(event: Dict[str, Any], is_warn: bool) -> Dict[str, Any]:
        """
        å°†äº‹ä»¶è§„èŒƒåŒ–ä¸ºæ¨ç†æœåŠ¡æ ¼å¼
        """
        try:
            # è·å–messageå¯¹è±¡ï¼ˆåŒ…å«è¯¦ç»†äº‹ä»¶ä¿¡æ¯ï¼‰
            message = event.get('message', {})
            
            # æå–äº‹ä»¶æ—¶é—´ - ä¼˜å…ˆä½¿ç”¨é¡¶å±‚timestampï¼Œæ ¼å¼åŒ–ä¸ºå­—ç¬¦ä¸²
            evt_time = event.get('timestamp', '')
            if not evt_time and 'evt.time' in message:
                # å¦‚æœæ²¡æœ‰timestampï¼Œä½¿ç”¨messageä¸­çš„evt.timeï¼ˆUnixæ—¶é—´æˆ³ï¼‰
                evt_time_unix = message.get('evt.time', '')
                if evt_time_unix:
                    try:
                        from datetime import datetime
                        evt_time = datetime.utcfromtimestamp(float(evt_time_unix)).isoformat() + 'Z'
                    except:
                        evt_time = str(evt_time_unix)
            
            # æå–äº‹ä»¶ç±»å‹ - ä¼˜å…ˆä½¿ç”¨é¡¶å±‚event_type
            evt_type = event.get('event_type', message.get('evt.type', ''))
            
            # æå–è¿›ç¨‹ä¿¡æ¯ - ä»messageä¸­è·å–
            proc_cmdline = message.get('proc.cmdline', '')
            proc_pid = str(message.get('proc.pid', ''))
            proc_ppid = str(message.get('proc.ppid', ''))
            proc_pcmdline = message.get('proc.pcmdline', '')
            
            # æå–æ–‡ä»¶æè¿°ç¬¦ä¿¡æ¯ - ä»messageä¸­è·å–
            fd_name = message.get('fd.name', '')
            
            # æå–ä¸»æœºä¿¡æ¯ - ä¼˜å…ˆä½¿ç”¨é¡¶å±‚host
            host = event.get('host', message.get('host', ''))
            
            normalized = {
                "evt.time": evt_time,
                "evt.type": evt_type,
                "proc.cmdline": proc_cmdline,
                "proc.pid": proc_pid,
                "proc.ppid": proc_ppid,
                "proc.pcmdline": proc_pcmdline,
                "fd.name": fd_name,
                "host": host,
                "is_warn": "true" if is_warn else "false"
            }
            
            return normalized
            
        except Exception as e:
            logger.error(f"äº‹ä»¶è§„èŒƒåŒ–å¤±è´¥: {e}", exc_info=True)
            return {}


class EventToAlertsProcessor(MapFunction):
    """äº‹ä»¶åˆ°å‘Šè­¦å¤„ç†å™¨ - ç”Ÿæˆå‘Šè­¦å¹¶å‡†å¤‡æ¨ç†æ•°æ®"""
    
    def __init__(self):
        self.rules_engine = get_rules_engine()
        self.event_normalizer = EventNormalizer()
        
    def map(self, value):
        try:
            event = json.loads(value)
            collector_id = event.get('collector_id', 'unknown')
            logger.info(f"ğŸ” å¤„ç†äº‹ä»¶: {event.get('event_type', 'unknown')} from {collector_id[:8]}")
            
            # ç¬¬ä¸€æ­¥: åŸºç¡€è§„åˆ™åŒ¹é…
            alerts = self.rules_engine.evaluate_event(event)
            is_warn = len(alerts) > 0
            
            if is_warn:
                logger.info(f"ğŸš¨ è§„åˆ™å¼•æ“åŒ¹é…åˆ° {len(alerts)} ä¸ªå‘Šè­¦")
            
            # ç¬¬äºŒæ­¥: ç”Ÿæˆå‘Šè­¦ï¼ˆä¸ä¾èµ–æ¨ç†ç»“æœï¼‰
            if alerts:
                alert = alerts[0]
                logger.info(f"ğŸš¨ ç”Ÿæˆå‘Šè­¦: {alert['alert']['id']} - {alert['alert']['rule']['name']}")
                return json.dumps(alert, ensure_ascii=False)
            
            return None
                
        except Exception as e:
            logger.error(f"å¤„ç†äº‹ä»¶å¼‚å¸¸: {e}")
            return None


class EventToInferenceRequestProcessor(MapFunction):
    """äº‹ä»¶è½¬æ¨ç†è¯·æ±‚å¤„ç†å™¨ - è§„èŒƒåŒ–äº‹ä»¶å¹¶å‡†å¤‡æ¨ç†è¯·æ±‚"""
    
    def __init__(self):
        self.rules_engine = get_rules_engine()
        self.event_normalizer = EventNormalizer()
        
        # æ¨ç†æœåŠ¡é…ç½®
        self.inference_threshold = float(os.getenv('INFERENCE_THRESHOLD', '0.5'))
        self.include_graph = True
        
    def map(self, value):
        try:
            event = json.loads(value)
            collector_id = event.get('collector_id', 'unknown')
            
            # è§„åˆ™åŒ¹é…åˆ¤æ–­æ˜¯å¦ä¸ºå‘Šè­¦äº‹ä»¶
            alerts = self.rules_engine.evaluate_event(event)
            is_warn = len(alerts) > 0
            
            # äº‹ä»¶è§„èŒƒåŒ–
            normalized_event = self.event_normalizer.normalize_event(event, is_warn)
            
            if normalized_event:
                # æ„å»ºæ¨ç†è¯·æ±‚
                inference_request = {
                    "collector_id": collector_id,
                    "events": [normalized_event],
                    "options": {
                        "threshold": self.inference_threshold,
                        "include_graph": self.include_graph
                    },
                    "timestamp": event.get('timestamp', ''),
                    "event_type": event.get('event_type', '')
                }
                
                logger.info(f"ğŸ“¤ ç”Ÿæˆæ¨ç†è¯·æ±‚: collector={collector_id[:8]}, is_warn={is_warn}")
                return json.dumps(inference_request, ensure_ascii=False)
            else:
                logger.warning(f"âš ï¸ äº‹ä»¶è§„èŒƒåŒ–å¤±è´¥ï¼Œè·³è¿‡æ¨ç†è¯·æ±‚")
                return None
                
        except Exception as e:
            logger.error(f"ç”Ÿæˆæ¨ç†è¯·æ±‚å¼‚å¸¸: {e}")
            return None


class AlertSeverityRouter(FilterFunction):
    """å‘Šè­¦ä¸¥é‡ç¨‹åº¦è·¯ç”±å™¨"""
    
    def __init__(self, target_severity: str = "high"):
        self.target_severity = target_severity
    
    def filter(self, value):
        try:
            alert = json.loads(value)
            severity = alert.get('alert', {}).get('severity', 'low')
            
            if self.target_severity == "high":
                return severity in ['high', 'critical']
            else:
                return severity in ['low', 'medium']
                
        except Exception:
            return False


def main():
    """ä¸»å‡½æ•°ï¼šåˆ›å»ºäº‹ä»¶åˆ°å‘Šè­¦çš„å¤„ç†ä½œä¸š"""
    
    logger.info("ğŸš€ Starting SysArmor Audit Events to Alerts Job")
    logger.info("ğŸ“‹ Based on Falco-style rule engine")
    logger.info("ğŸ“Š Processing: sysarmor.events.audit â†’ alerts + inference requests")
    
    # ç¯å¢ƒå˜é‡é…ç½®
    kafka_servers = os.getenv('KAFKA_BOOTSTRAP_SERVERS', 'middleware-kafka:9092')
    input_topic = 'sysarmor.events.audit'
    output_alerts_topic = 'sysarmor.alerts.audit'  # å‘Šè­¦è¾“å‡ºtopic
    output_inference_topic = 'sysarmor.inference.requests'  # æ¨ç†è¯·æ±‚è¾“å‡ºtopic
    kafka_group_id = 'sysarmor-audit-events-to-alerts-processor'
    
    # æ¨ç†æœåŠ¡é…ç½®
    inference_threshold = os.getenv('INFERENCE_THRESHOLD', '0.5')
    
    logger.info(f"ğŸ“¡ Kafka Servers: {kafka_servers}")
    logger.info(f"ğŸ“¥ Input Topic: {input_topic}")
    logger.info(f"ğŸ“¤ Output Alerts Topic: {output_alerts_topic}")
    logger.info(f"ğŸ“¤ Output Inference Topic: {output_inference_topic}")
    logger.info(f"ğŸ‘¥ Consumer Group: {kafka_group_id}")
    logger.info(f"ğŸ¯ Inference Threshold: {inference_threshold}")
    logger.info("")
    
    # åˆ›å»ºæµå¤„ç†ç¯å¢ƒ
    env = StreamExecutionEnvironment.get_execution_environment()
    
    # é…ç½®ç¯å¢ƒ
    env.set_parallelism(2)  # 2ä¸ªå¹¶è¡Œåº¦
    env.enable_checkpointing(30000)  # 30ç§’ checkpoint
    env.get_checkpoint_config().set_checkpointing_mode(CheckpointingMode.EXACTLY_ONCE)
    
    try:
        # æ·»åŠ  JAR ä¾èµ–
        env.add_jars("file:///opt/flink/lib/flink-sql-connector-kafka-3.1.0-1.18.jar")
        
        # åˆ›å»º Kafka Consumer
        consumer_props = {
            'bootstrap.servers': kafka_servers,
            'group.id': kafka_group_id,
            'auto.offset.reset': 'earliest',  # å¤„ç†æ‰€æœ‰äº‹ä»¶ï¼ŒåŒ…æ‹¬å†å²äº‹ä»¶
            'session.timeout.ms': '30000',
            'heartbeat.interval.ms': '10000',
            'max.poll.interval.ms': '300000'
        }
        
        kafka_consumer = FlinkKafkaConsumer(
            topics=[input_topic],
            deserialization_schema=SimpleStringSchema(),
            properties=consumer_props
        )
        
        # åˆ›å»º Kafka Producer (å‘Šè­¦æµ)
        producer_props = {
            'bootstrap.servers': kafka_servers,
            'transaction.timeout.ms': '900000',
            'batch.size': '16384',
            'linger.ms': '5',
            'compression.type': 'snappy'
        }
        
        kafka_alerts_producer = FlinkKafkaProducer(
            topic=output_alerts_topic,
            serialization_schema=SimpleStringSchema(),
            producer_config=producer_props
        )
        
        # åˆ›å»º Kafka Producer (æ¨ç†è¯·æ±‚æµ)
        kafka_inference_producer = FlinkKafkaProducer(
            topic=output_inference_topic,
            serialization_schema=SimpleStringSchema(),
            producer_config=producer_props
        )
        
        logger.info("ğŸ“‹ Creating Falco-style threat detection pipeline...")
        
        # æ„å»ºæ•°æ®æµå¤„ç†ç®¡é“
        events_stream = env.add_source(kafka_consumer)
        
        # æŒ‰ collector_id åˆ†ç»„ï¼Œæ”¯æŒé¢‘ç‡æ£€æµ‹
        keyed_stream = events_stream.key_by(
            lambda event: json.loads(event).get('collector_id', 'unknown')
        )
        
        # å¨èƒæ£€æµ‹å¤„ç† - ç”Ÿæˆå‘Šè­¦
        alerts_stream = events_stream.map(
            EventToAlertsProcessor(),
            output_type=Types.STRING()
        ).filter(lambda x: x is not None)
        
        # æ¨ç†è¯·æ±‚å¤„ç† - ç”Ÿæˆæ¨ç†è¯·æ±‚
        inference_stream = events_stream.map(
            EventToInferenceRequestProcessor(),
            output_type=Types.STRING()
        ).filter(lambda x: x is not None)
        
        # é…ç½® OpenSearch sink
        opensearch_url = os.getenv('OPENSEARCH_URL', 'http://opensearch:9200')
        opensearch_username = os.getenv('OPENSEARCH_USERNAME', 'admin')
        opensearch_password = os.getenv('OPENSEARCH_PASSWORD', 'admin')
        
        logger.info(f"ğŸ” OpenSearch URL: {opensearch_url}")
        
        # åˆ›å»º OpenSearch HTTP Sink (å€Ÿé‰´å·¥ä½œç‰ˆæœ¬çš„æ–¹æ¡ˆ)
        class OpenSearchHttpSink(MapFunction):
            """OpenSearch HTTP Sink - ä½¿ç”¨ HTTP è¯·æ±‚å†™å…¥"""
            
            def __init__(self):
                self.opensearch_url = opensearch_url
                self.opensearch_username = opensearch_username
                self.opensearch_password = opensearch_password
                self.index_url = f"{opensearch_url}/sysarmor-alerts-audit/_doc"
                
            def map(self, value):
                try:
                    if not value:
                        return value
                    
                    alert_data = json.loads(value)
                    
                    headers = {'Content-Type': 'application/json'}
                    auth = (self.opensearch_username, self.opensearch_password)
                    
                    response = requests.post(
                        self.index_url,
                        json=alert_data,
                        headers=headers,
                        auth=auth,
                        timeout=10,
                        verify=False
                    )
                    
                    if response.status_code in [200, 201]:
                        logger.info(f"âœ… å‘Šè­¦å†™å…¥ OpenSearch: {alert_data.get('alert', {}).get('id', 'unknown')}")
                    else:
                        logger.error(f"âŒ OpenSearch å†™å…¥å¤±è´¥: {response.status_code}")
                        
                except Exception as e:
                    logger.error(f"âŒ OpenSearch HTTP Sink é”™è¯¯: {e}")
                
                return value
        
        opensearch_http_sink = OpenSearchHttpSink()
        logger.info("âœ… OpenSearch HTTP sink å·²é…ç½®: sysarmor-alerts-audit")
        
        # å‘Šè­¦è¾“å‡ºåˆ° Kafka
        alerts_stream.add_sink(kafka_alerts_producer)
        logger.info(f"âœ… å‘Šè­¦å°†å†™å…¥ Kafka Topic: {output_alerts_topic}")
        
        # æ¨ç†è¯·æ±‚è¾“å‡ºåˆ° Kafka
        inference_stream.add_sink(kafka_inference_producer)
        logger.info(f"âœ… æ¨ç†è¯·æ±‚å°†å†™å…¥ Kafka Topic: {output_inference_topic}")
        
        # æ‰€æœ‰å‘Šè­¦å†™å…¥ OpenSearch (ä½¿ç”¨ HTTP æ–¹å¼)
        alerts_stream.map(opensearch_http_sink, output_type=Types.STRING())
        logger.info("âœ… æ‰€æœ‰å‘Šè­¦å°†å†™å…¥ OpenSearch: sysarmor-alerts-audit")
        
        # ç›‘æ§è¾“å‡º - å‘Šè­¦
        alerts_stream.map(
            lambda x: f"ğŸš¨ Alert: {json.loads(x).get('alert', {}).get('severity', 'unknown')} - {json.loads(x).get('alert', {}).get('rule', {}).get('name', 'unknown')} from {json.loads(x).get('metadata', {}).get('collector_id', 'unknown')[:8]}",
            output_type=Types.STRING()
        ).print()
        
        # ç›‘æ§è¾“å‡º - æ¨ç†è¯·æ±‚
        inference_stream.map(
            lambda x: f"ğŸ“¦ Inference Request: collector={json.loads(x).get('collector_id', 'unknown')[:8]}, events={len(json.loads(x).get('events', []))}",
            output_type=Types.STRING()
        ).print()
        
        logger.info("ğŸ”„ Falco-style threat detection pipeline created:")
        logger.info(f"   {input_topic} -> Rule Engine -> Alerts -> {output_alerts_topic}")
        logger.info(f"   {input_topic} -> Normalizer -> Inference Requests -> {output_inference_topic}")
        
        # æ˜¾ç¤ºåŠ è½½çš„è§„åˆ™ï¼ˆä½¿ç”¨å…¨å±€å•ä¾‹ï¼‰
        rules_engine = get_rules_engine()
        logger.info(f"ğŸ›¡ï¸ å¨èƒæ£€æµ‹è§„åˆ™: {len(rules_engine.rules)} ä¸ªè§„åˆ™å·²åŠ è½½")
        
        logger.info("ğŸ¯ æ•°æ®è¾“å‡º:")
        logger.info(f"   - å‘Šè­¦ Kafka Topic: {output_alerts_topic}")
        logger.info(f"   - æ¨ç†è¯·æ±‚ Kafka Topic: {output_inference_topic}")
        logger.info(f"   - OpenSearchç´¢å¼•: sysarmor-alerts-audit")
        
        # æ‰§è¡Œä½œä¸š
        logger.info("âœ… Starting audit threat detection job...")
        
        job_client = env.execute_async("SysArmor-Audit-Events-to-Alerts-Processor")
        
        logger.info(f"ğŸ¯ Audit Events to Alerts job submitted successfully!")
        logger.info(f"ğŸ“‹ Job submitted with async execution")
        logger.info(f"ğŸŒ Monitor at: http://localhost:8081")
        logger.info(f"ğŸ“Š Processing:")
        logger.info(f"   - {input_topic} â†’ {output_alerts_topic} (Alerts)")
        logger.info(f"   - {input_topic} â†’ {output_inference_topic} (Inference Requests)")
        logger.info(f"ğŸ” View logs: docker logs -f sysarmor-flink-taskmanager-1")
        
        return "async-job-submitted"
        
    except Exception as e:
        logger.error(f"âŒ Events to Alerts job failed: {e}")
        raise


if __name__ == "__main__":
    main()
