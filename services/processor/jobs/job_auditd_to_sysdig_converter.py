#!/usr/bin/env python3
"""
SysArmor Processor - Auditd to Sysdig Format Converter
å°†auditdæ ¼å¼çš„æ•°æ®è½¬æ¢ä¸ºsysdigæ ¼å¼ï¼Œç”¨äºåç»­çš„å¨èƒæ£€æµ‹å’Œåˆ†æ
"""

import os
import json
import logging
import re
from datetime import datetime
from typing import Dict, List, Optional, Any
from pyflink.datastream import StreamExecutionEnvironment, CheckpointingMode
from pyflink.datastream.connectors.kafka import FlinkKafkaConsumer, FlinkKafkaProducer
from pyflink.common.serialization import SimpleStringSchema
from pyflink.common.typeinfo import Types
from pyflink.datastream.functions import MapFunction, FilterFunction
from pyflink.common import Configuration

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class AuditdParser:
    """Auditdæ—¥å¿—è§£æå™¨"""
    
    # ç³»ç»Ÿè°ƒç”¨å·åˆ°äº‹ä»¶ç±»å‹çš„æ˜ å°„
    SYSCALL_MAP = {
        0: "read", 1: "write", 2: "open", 3: "close", 4: "stat", 5: "fstat",
        6: "lstat", 7: "poll", 8: "lseek", 9: "mmap", 10: "mprotect", 11: "munmap",
        12: "brk", 13: "rt_sigaction", 14: "rt_sigprocmask", 15: "rt_sigreturn",
        16: "ioctl", 17: "pread64", 18: "pwrite64", 19: "readv", 20: "writev",
        21: "access", 22: "pipe", 23: "select", 24: "sched_yield", 25: "mremap",
        26: "msync", 27: "mincore", 28: "madvise", 29: "shmget", 30: "shmat",
        31: "shmctl", 32: "dup", 33: "dup2", 34: "pause", 35: "nanosleep",
        36: "getitimer", 37: "alarm", 38: "setitimer", 39: "getpid", 40: "sendfile",
        41: "socket", 42: "connect", 43: "accept", 44: "sendto", 45: "recvfrom",
        46: "sendmsg", 47: "recvmsg", 48: "shutdown", 49: "bind", 50: "listen",
        51: "getsockname", 52: "getpeername", 53: "socketpair", 54: "setsockopt",
        55: "getsockopt", 56: "clone", 57: "fork", 58: "vfork", 59: "execve",
        60: "exit", 61: "wait4", 62: "kill", 63: "uname", 64: "semget",
        65: "semop", 66: "semctl", 67: "shmdt", 68: "msgget", 69: "msgsnd",
        70: "msgrcv", 71: "msgctl", 72: "fcntl", 73: "flock", 74: "fsync",
        75: "fdatasync", 76: "truncate", 77: "ftruncate", 78: "getdents",
        79: "getcwd", 80: "chdir", 81: "fchdir", 82: "rename", 83: "mkdir",
        84: "rmdir", 85: "creat", 86: "link", 87: "unlink", 88: "symlink",
        89: "readlink", 90: "chmod", 91: "fchmod", 92: "chown", 93: "fchown",
        94: "lchown", 95: "umask", 96: "gettimeofday", 97: "getrlimit",
        98: "getrusage", 99: "sysinfo", 100: "times", 257: "openat"
    }
    
    # NODLINKæ”¯æŒçš„äº‹ä»¶ç±»å‹
    SUPPORTED_EVENTS = {
        "read", "readv", "write", "writev", "fcntl", "rmdir", "rename", "chmod",
        "execve", "clone", "pipe", "fork", "accept", "sendmsg", "recvmsg", 
        "recvfrom", "send", "sendto", "open", "openat", "socket", "connect"
    }
    
    def __init__(self):
        self.process_cache = {}  # è¿›ç¨‹ç¼“å­˜ï¼Œç”¨äºè¿›ç¨‹æ ‘é‡å»º
        
    def parse_auditd_line(self, line: str) -> Optional[Dict[str, Any]]:
        """è§£æå•è¡Œauditdæ—¥å¿—"""
        try:
            # æ­£åˆ™åŒ¹é…auditdæ ¼å¼
            match = re.match(r'type=([^ ]+) msg=audit\(([\d.]+):(\d+)\): (.*)', line)
            if not match:
                return None
                
            record_type, timestamp, event_id, fields_str = match.groups()
            
            # è§£æå­—æ®µ
            fields = {}
            field_pattern = r'(\w+)=("[^"]*"|\S+)'
            for field_match in re.finditer(field_pattern, fields_str):
                key, value = field_match.groups()
                # å»é™¤å¼•å·
                if value.startswith('"') and value.endswith('"'):
                    value = value[1:-1]
                fields[key] = value
            
            return {
                'type': record_type,
                'timestamp': float(timestamp),
                'event_id': event_id,
                'fields': fields
            }
        except Exception as e:
            logger.warning(f"Failed to parse auditd line: {line}, error: {e}")
            return None
    
    def decode_cmdline(self, hex_str: str) -> str:
        """è§£ç å‘½ä»¤è¡Œï¼ˆå¯èƒ½æ˜¯åå…­è¿›åˆ¶ç¼–ç ï¼‰"""
        if not hex_str:
            return ""
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºåå…­è¿›åˆ¶å­—ç¬¦ä¸²
        if not all(c in '0123456789abcdefABCDEF' for c in hex_str):
            return hex_str  # éåå…­è¿›åˆ¶ï¼Œç›´æ¥è¿”å›
        
        if len(hex_str) % 2 != 0:
            return hex_str  # é•¿åº¦å¿…é¡»ä¸ºå¶æ•°
        
        try:
            hex_bytes = bytes.fromhex(hex_str)
            parts = hex_bytes.split(b'\x00')
            return b' '.join(part for part in parts if part).decode('utf-8', errors='replace').strip()
        except:
            return hex_str  # è§£ç å¤±è´¥ï¼Œè¿”å›åŸå­—ç¬¦ä¸²
    
    def convert_to_sysdig(self, audit_records: List[Dict[str, Any]]) -> Optional[Dict[str, Any]]:
        """å°†åˆ†ç»„çš„auditè®°å½•è½¬æ¢ä¸ºsysdigæ ¼å¼"""
        try:
            # æŸ¥æ‰¾SYSCALLè®°å½•
            syscall_record = None
            path_records = []
            proctitle_record = None
            
            for record in audit_records:
                if record['type'] == 'SYSCALL':
                    syscall_record = record
                elif record['type'] == 'PATH':
                    path_records.append(record)
                elif record['type'] == 'PROCTITLE':
                    proctitle_record = record
            
            if not syscall_record:
                return None
            
            fields = syscall_record['fields']
            
            # è·å–ç³»ç»Ÿè°ƒç”¨ç±»å‹
            syscall_num = int(fields.get('syscall', '0'))
            evt_type = self.SYSCALL_MAP.get(syscall_num, f"syscall_{syscall_num}")
            
            # åªå¤„ç†æ”¯æŒçš„äº‹ä»¶ç±»å‹
            if evt_type not in self.SUPPORTED_EVENTS:
                return None
            
            # æ„å»ºsysdigæ ¼å¼äº‹ä»¶
            sysdig_event = {
                "evt.num": int(syscall_record['event_id']),
                "evt.time": syscall_record['timestamp'],
                "evt.type": evt_type,
                "evt.category": self._get_event_category(evt_type),
                "proc.name": fields.get('comm', '').strip('"'),
                "proc.exe": fields.get('exe', '').strip('"'),
                "proc.pid": int(fields.get('pid', '0')),
                "proc.ppid": int(fields.get('ppid', '0')),
                "is_warn": False
            }
            
            # å¤„ç†å‘½ä»¤è¡Œ
            if proctitle_record and 'proctitle' in proctitle_record['fields']:
                cmdline = self.decode_cmdline(proctitle_record['fields']['proctitle'])
                sysdig_event["proc.cmdline"] = cmdline
            else:
                sysdig_event["proc.cmdline"] = sysdig_event["proc.name"]
            
            # å¤„ç†æ–‡ä»¶è·¯å¾„
            if path_records:
                # å–ç¬¬ä¸€ä¸ªæœ‰æ•ˆè·¯å¾„
                for path_record in path_records:
                    if 'name' in path_record['fields']:
                        fd_name = path_record['fields']['name'].strip('"')
                        if fd_name and fd_name != '(null)':
                            sysdig_event["fd.name"] = fd_name
                            break
            
            # å¤„ç†ç½‘ç»œäº‹ä»¶
            if evt_type in ['socket', 'connect', 'accept', 'sendto', 'recvfrom']:
                self._add_network_fields(sysdig_event, fields)
            
            # æ›´æ–°è¿›ç¨‹ç¼“å­˜
            self._update_process_cache(sysdig_event)
            
            # æ·»åŠ çˆ¶è¿›ç¨‹å‘½ä»¤è¡Œ
            parent_cmdline = self._get_parent_cmdline(
                sysdig_event["proc.ppid"], 
                sysdig_event["evt.time"]
            )
            sysdig_event["proc.pcmdline"] = parent_cmdline
            
            return sysdig_event
            
        except Exception as e:
            logger.error(f"Failed to convert audit records to sysdig: {e}")
            return None
    
    def _get_event_category(self, evt_type: str) -> str:
        """è·å–äº‹ä»¶ç±»åˆ«"""
        file_events = {"read", "readv", "write", "writev", "open", "openat", "close", "fcntl"}
        process_events = {"execve", "clone", "fork", "exit"}
        network_events = {"socket", "connect", "accept", "sendto", "recvfrom", "sendmsg", "recvmsg"}
        
        if evt_type in file_events:
            return "file"
        elif evt_type in process_events:
            return "process"
        elif evt_type in network_events:
            return "net"
        else:
            return "other"
    
    def _add_network_fields(self, event: Dict[str, Any], fields: Dict[str, str]):
        """æ·»åŠ ç½‘ç»œç›¸å…³å­—æ®µ"""
        # è¿™é‡Œå¯ä»¥æ ¹æ®éœ€è¦æ·»åŠ ç½‘ç»œåœ°å€ã€ç«¯å£ç­‰ä¿¡æ¯
        if 'saddr' in fields:
            event["fd.sip"] = fields['saddr']
        if 'daddr' in fields:
            event["fd.dip"] = fields['daddr']
        if 'sport' in fields:
            event["fd.sport"] = int(fields['sport'])
        if 'dport' in fields:
            event["fd.dport"] = int(fields['dport'])
    
    def _update_process_cache(self, event: Dict[str, Any]):
        """æ›´æ–°è¿›ç¨‹ç¼“å­˜"""
        pid = event["proc.pid"]
        self.process_cache[pid] = {
            'cmdline': event["proc.cmdline"],
            'timestamp': event["evt.time"]
        }
    
    def _get_parent_cmdline(self, ppid: int, event_time: float) -> str:
        """è·å–çˆ¶è¿›ç¨‹å‘½ä»¤è¡Œ"""
        if not ppid:
            return ""
        
        # ç³»ç»Ÿè¿›ç¨‹æ˜ å°„
        system_processes = {
            1: 'systemd --system --deserialize',
            2: '[kthreadd]',
            0: ''
        }
        
        if ppid in system_processes:
            return system_processes[ppid]
        
        # ä»ç¼“å­˜ä¸­æŸ¥æ‰¾
        if ppid in self.process_cache:
            cached_info = self.process_cache[ppid]
            # æ£€æŸ¥æ—¶é—´çª—å£ï¼ˆÂ±60ç§’ï¼‰
            if abs(cached_info['timestamp'] - event_time) <= 60:
                return cached_info['cmdline']
        
        return ""  # æ— æ³•é‡å»º


class AuditdToSysdigConverter(MapFunction):
    """Auditdåˆ°Sysdigæ ¼å¼è½¬æ¢å™¨"""
    
    def __init__(self):
        self.parser = AuditdParser()
        self.event_buffer = {}  # æŒ‰event_idåˆ†ç»„çš„ç¼“å†²åŒº
        
    def map(self, value):
        try:
            if not value:
                return None
                
            # è§£æè¾“å…¥çš„JSONæ¶ˆæ¯
            data = json.loads(value)
            message = data.get('message', '')
            host = data.get('host', 'unknown')
            
            # è§£æauditdæ—¥å¿—è¡Œ
            audit_record = self.parser.parse_auditd_line(message)
            if not audit_record:
                return None
            
            event_id = audit_record['event_id']
            
            # å°†è®°å½•æ·»åŠ åˆ°ç¼“å†²åŒº
            if event_id not in self.event_buffer:
                self.event_buffer[event_id] = []
            self.event_buffer[event_id].append(audit_record)
            
            # æ£€æŸ¥æ˜¯å¦å¯ä»¥å¤„ç†è¿™ä¸ªäº‹ä»¶ç»„
            # ç®€åŒ–å¤„ç†ï¼šå¦‚æœæœ‰SYSCALLè®°å½•å°±å°è¯•è½¬æ¢
            has_syscall = any(r['type'] == 'SYSCALL' for r in self.event_buffer[event_id])
            
            if has_syscall:
                # è½¬æ¢ä¸ºsysdigæ ¼å¼
                sysdig_event = self.parser.convert_to_sysdig(self.event_buffer[event_id])
                
                # æ¸…ç†ç¼“å†²åŒº
                del self.event_buffer[event_id]
                
                if sysdig_event:
                    # æ·»åŠ ä¸»æœºä¿¡æ¯
                    sysdig_event["host"] = host
                    
                    logger.debug(f"Converted event: {sysdig_event['evt.type']} from {sysdig_event['proc.name']}")
                    return json.dumps(sysdig_event, ensure_ascii=False)
            
            return None
            
        except Exception as e:
            logger.error(f"Error in AuditdToSysdigConverter: {e}")
            return None


class SysdigEventFilter(FilterFunction):
    """è¿‡æ»¤æœ‰æ•ˆçš„Sysdigäº‹ä»¶"""
    
    def filter(self, value):
        try:
            if not value:
                return False
            
            event = json.loads(value)
            
            # æ£€æŸ¥å¿…éœ€å­—æ®µ
            required_fields = ['evt.type', 'proc.pid', 'proc.name']
            for field in required_fields:
                if field not in event:
                    return False
            
            # æ£€æŸ¥äº‹ä»¶ç±»å‹æ˜¯å¦æ”¯æŒ
            if event['evt.type'] not in AuditdParser.SUPPORTED_EVENTS:
                return False
            
            return True
            
        except Exception as e:
            logger.error(f"Error in SysdigEventFilter: {e}")
            return False


def generate_output_topic(input_topic: str) -> str:
    """æ ¹æ®è¾“å…¥topicç”Ÿæˆå¯¹åº”çš„è¾“å‡ºtopic"""
    # å°† sysarmor-agentless-xxx è½¬æ¢ä¸º sysarmor-sysdig-xxx
    if input_topic.startswith('sysarmor-agentless-'):
        collector_id = input_topic.replace('sysarmor-agentless-', '')
        return f'sysarmor-sysdig-{collector_id}'
    else:
        # å¦‚æœä¸æ˜¯æ ‡å‡†æ ¼å¼ï¼Œæ·»åŠ -sysdigåç¼€
        return f'{input_topic}-sysdig'

def main():
    """ä¸»å‡½æ•°ï¼šåˆ›å»ºAuditdåˆ°Sysdigè½¬æ¢ä½œä¸š"""
    
    logger.info("ğŸš€ Starting SysArmor Auditd to Sysdig Converter Job")
    
    # ç¯å¢ƒå˜é‡
    kafka_servers = os.getenv('KAFKA_BOOTSTRAP_SERVERS', 'middleware-kafka:9092')
    input_topic = os.getenv('INPUT_TOPIC', 'sysarmor-agentless-558c01dd')
    
    # å¦‚æœæ²¡æœ‰æ˜ç¡®æŒ‡å®šè¾“å‡ºtopicï¼Œåˆ™æ ¹æ®è¾“å…¥topicè‡ªåŠ¨ç”Ÿæˆ
    output_topic = os.getenv('OUTPUT_TOPIC')
    if not output_topic:
        output_topic = generate_output_topic(input_topic)
        logger.info(f"ğŸ”„ Auto-generated output topic: {output_topic}")
    
    kafka_group_id = os.getenv('KAFKA_GROUP_ID', 'sysarmor-auditd-converter-group')
    
    logger.info(f"ğŸ“¡ Kafka Servers: {kafka_servers}")
    logger.info(f"ğŸ“¥ Input Topic: {input_topic}")
    logger.info(f"ğŸ“¤ Output Topic: {output_topic}")
    logger.info(f"ğŸ‘¥ Consumer Group: {kafka_group_id}")
    
    # åˆ›å»ºæµå¤„ç†ç¯å¢ƒ
    env = StreamExecutionEnvironment.get_execution_environment()
    
    # é…ç½®ç¯å¢ƒ
    env.set_parallelism(2)  # è®¾ç½®å¹¶è¡Œåº¦
    env.enable_checkpointing(60000)  # 60ç§’ checkpoint
    env.get_checkpoint_config().set_checkpointing_mode(CheckpointingMode.EXACTLY_ONCE)
    
    try:
        # æ·»åŠ  JAR ä¾èµ–
        env.add_jars("file:///opt/flink/lib/flink-sql-connector-kafka-3.1.0-1.18.jar")
        
        # åˆ›å»º Kafka Consumer
        kafka_props = {
            'bootstrap.servers': kafka_servers,
            'group.id': kafka_group_id,
            'auto.offset.reset': 'latest',
            'client.dns.lookup': 'use_all_dns_ips',
            'session.timeout.ms': '30000',
            'heartbeat.interval.ms': '10000',
            'max.poll.interval.ms': '300000',
            'connections.max.idle.ms': '540000',
            'request.timeout.ms': '30000'
        }
        
        kafka_consumer = FlinkKafkaConsumer(
            topics=[input_topic],
            deserialization_schema=SimpleStringSchema(),
            properties=kafka_props
        )
        
        # åˆ›å»º Kafka Producer
        kafka_producer = FlinkKafkaProducer(
            topic=output_topic,
            serialization_schema=SimpleStringSchema(),
            producer_config=kafka_props
        )
        
        logger.info("ğŸ“‹ Creating conversion pipeline...")
        
        # æ„å»ºæ•°æ®æµå¤„ç†ç®¡é“
        converted_stream = env.add_source(kafka_consumer) \
            .map(AuditdToSysdigConverter(), output_type=Types.STRING()) \
            .filter(SysdigEventFilter())
        
        # è¾“å‡ºåˆ°Kafka
        converted_stream.add_sink(kafka_producer)
        
        logger.info("ğŸ”„ Conversion pipeline created:")
        logger.info("   Kafka Source (Auditd) -> Auditd Parser -> Sysdig Converter -> Filter -> Kafka Sink (Sysdig)")
        logger.info("ğŸ¯ Supported event types:")
        for evt_type in sorted(AuditdParser.SUPPORTED_EVENTS):
            logger.info(f"   - {evt_type}")
        
        logger.info("ğŸ›¡ï¸ Features:")
        logger.info("   - Real-time auditd to sysdig conversion")
        logger.info("   - Process tree reconstruction")
        logger.info("   - Command line decoding")
        logger.info("   - Event filtering and validation")
        
        # æ‰§è¡Œä½œä¸š
        logger.info("âœ… Starting Auditd to Sysdig conversion job...")
        
        job_client = env.execute_async("SysArmor-Auditd-To-Sysdig-Converter")
        job_id = job_client.get_job_id()
        
        logger.info(f"ğŸ¯ Conversion job submitted successfully!")
        logger.info(f"ğŸ“‹ Job ID: {job_id}")
        logger.info(f"ğŸŒ Monitor at: http://localhost:8081")
        logger.info(f"ğŸ“Š Converting from {input_topic} to {output_topic}")
        
        return job_id
        
    except Exception as e:
        logger.error(f"âŒ Conversion job failed: {e}")
        raise


if __name__ == "__main__":
    main()
