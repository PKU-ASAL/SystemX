#!/usr/bin/env python3
"""
SysArmor Processor - ç®€å•æ§åˆ¶å°æµ‹è¯•ä½œä¸š
è¯»å–æŒ‡å®š topic prefix çš„æ•°æ®å¹¶è¾“å‡ºåˆ°æ§åˆ¶å°ï¼Œç”¨äºéªŒè¯ Flink åŸºæœ¬åŠŸèƒ½
"""

import os
import json
import logging
from datetime import datetime
from pyflink.datastream import StreamExecutionEnvironment
from pyflink.datastream.connectors.kafka import FlinkKafkaConsumer
from pyflink.common.serialization import SimpleStringSchema
from pyflink.common.typeinfo import Types
from pyflink.datastream.functions import MapFunction

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class SimpleConsoleOutput(MapFunction):
    """ç®€å•çš„æ§åˆ¶å°è¾“å‡ºå‡½æ•°"""
    
    def __init__(self):
        self.message_count = 0
        
    def map(self, value):
        try:
            self.message_count += 1
            
            # å°è¯•è§£æ JSON
            try:
                data = json.loads(value)
                timestamp = data.get('timestamp', 'N/A')
                host = data.get('host', 'N/A')
                message = data.get('message', 'N/A')
                collector_id = data.get('collector_id', 'N/A')
                
                # æ ¼å¼åŒ–è¾“å‡º
                output = f"ğŸ” MESSAGE #{self.message_count} | Time: {timestamp} | Host: {host} | Collector: {collector_id[:8]}... | Content: {message[:100]}..."
                
            except json.JSONDecodeError:
                # å¦‚æœä¸æ˜¯ JSONï¼Œç›´æ¥è¾“å‡ºåŸå§‹å†…å®¹
                output = f"ğŸ” RAW MESSAGE #{self.message_count} | Content: {value[:150]}..."
            
            # è¾“å‡ºåˆ°æ§åˆ¶å°
            print(output)
            logger.info(f"Processed message #{self.message_count}")
            
            return value
            
        except Exception as e:
            logger.error(f"Error processing message: {e}")
            print(f"âŒ ERROR #{self.message_count}: {str(e)}")
            return value

def main():
    """ä¸»å‡½æ•°ï¼šåˆ›å»ºç®€å•çš„æ§åˆ¶å°æµ‹è¯•ä½œä¸š"""
    
    logger.info("ğŸš€ Starting SysArmor Simple Console Test Job")
    
    # ç¯å¢ƒå˜é‡é…ç½®
    kafka_servers = os.getenv('KAFKA_BOOTSTRAP_SERVERS', 'localhost:9094')
    topic_prefix = os.getenv('TOPIC_PREFIX', 'sysarmor-')
    kafka_group_id = os.getenv('KAFKA_GROUP_ID', f'sysarmor-console-test-group-{datetime.now().strftime("%Y%m%d-%H%M%S")}')
    
    # å¯ä»¥é€šè¿‡ç¯å¢ƒå˜é‡æŒ‡å®šå…·ä½“çš„ topicsï¼Œç”¨é€—å·åˆ†éš”
    specific_topics = os.getenv('TEST_TOPICS', '')
    
    if specific_topics:
        topics = [topic.strip() for topic in specific_topics.split(',')]
        logger.info(f"ğŸ“‹ Using specific topics: {topics}")
    else:
        # åªæ¶ˆè´¹æµ‹è¯• topic
        topics = ['sysarmor-events-test']
        logger.info(f"ğŸ“‹ Using test topic: {topics}")
    
    logger.info(f"ğŸ“¡ Kafka Servers: {kafka_servers}")
    logger.info(f"ğŸ‘¥ Consumer Group: {kafka_group_id}")
    logger.info(f"ğŸ¯ Topic Prefix Filter: {topic_prefix}")
    
    # åˆ›å»ºæµå¤„ç†ç¯å¢ƒ
    env = StreamExecutionEnvironment.get_execution_environment()
    
    # é…ç½®ç¯å¢ƒ - ç®€å•é…ç½®ä¾¿äºè°ƒè¯•
    env.set_parallelism(1)  # å•å¹¶è¡Œåº¦ï¼Œä¾¿äºè§‚å¯Ÿè¾“å‡ºé¡ºåº
    env.enable_checkpointing(30000)  # 30ç§’ checkpoint
    
    try:
        # æ·»åŠ  Kafka JAR ä¾èµ–
        env.add_jars("file:///opt/flink/lib/flink-sql-connector-kafka-3.1.0-1.18.jar")
        
        # Kafka è¿æ¥é…ç½®
        kafka_props = {
            'bootstrap.servers': kafka_servers,
            'group.id': kafka_group_id,
            'auto.offset.reset': 'earliest',  # ä»æœ€æ—©æ¶ˆæ¯å¼€å§‹è¯»å–
            'session.timeout.ms': '30000',
            'heartbeat.interval.ms': '10000',
            'max.poll.interval.ms': '300000',
            'enable.auto.commit': 'true',
            'auto.commit.interval.ms': '5000'
        }
        
        # åˆ›å»º Kafka Consumer
        kafka_consumer = FlinkKafkaConsumer(
            topics=topics,
            deserialization_schema=SimpleStringSchema(),
            properties=kafka_props
        )
        
        logger.info("ğŸ“‹ Creating simple console test pipeline...")
        
        # æ„å»ºç®€å•çš„æ•°æ®æµå¤„ç†ç®¡é“
        message_stream = env.add_source(kafka_consumer) \
            .map(SimpleConsoleOutput(), output_type=Types.STRING())
        
        logger.info("ğŸ” Simple test pipeline created:")
        logger.info("   Kafka Source -> Console Output")
        logger.info("ğŸ¯ Features:")
        logger.info("   - Real-time message display")
        logger.info("   - JSON parsing with fallback")
        logger.info("   - Message counter")
        logger.info("   - Error handling")
        
        logger.info("âœ… Starting simple console test job...")
        logger.info("ğŸ–¥ï¸  Messages will appear in TaskManager logs")
        logger.info("ğŸ“Š Monitor at: http://localhost:8081")
        logger.info("ğŸ” Look for 'ğŸ” MESSAGE #' in logs")
        
        # æ‰§è¡Œä½œä¸š
        job_client = env.execute_async("SysArmor-Simple-Console-Test")
        job_id = job_client.get_job_id()
        
        logger.info(f"ğŸ¯ Simple test job submitted successfully!")
        logger.info(f"ğŸ“‹ Job ID: {job_id}")
        logger.info(f"ğŸŒ Monitor at: http://localhost:8081")
        logger.info(f"ğŸ“ Check TaskManager logs for console output")
        
        return job_id
        
    except Exception as e:
        logger.error(f"âŒ Simple test job failed: {e}")
        raise

if __name__ == "__main__":
    main()
